---
title: "Posterior Approximation - Incomplete"
author: "Adrien Osakwe"
format: html
editor: visual
---

Previously we have looked at Bayesian models with closed form solutions for the marginal likelihood and posterior. However, in cases where more complex models are required, the marginal likelihood is often not closed form making it impossible to solve the posterior analytically.

In this cases, we are required to **approximate** the posterior from which we can then compute thestatistics of interest which we saw in the previous section.

There are two key ways to approximate the posterior:

1.  Sampling/Simulation: we can generate random samples from the posterior distribution to then use the empirical results as an estimate of the posterior.
2.  Variational Inference: we can approximate the posterior by using a closed-form surrogate distribution.

A simple form of variational inference is normal approximation, where the central limit theorem is used to justify the use of the normal distribution as a surrogate posterior. In this chapter we will focus on sampling methods for posterior approximation.

### Sampling Methods

To approximate the posterior by simulation, we need to generate random samples from the posterior. This is trivial if the posterior is a known distribution with a derived simulation method. However, this becomes interesting when the posterior **isn't** closed form. We can achieve this by making use of the unnormalized posterior density or any function $q(\theta;y)$ that is proportional to it.:

$$
p(\theta|y) \propto p(\theta)p(y|\theta) \propto q(\theta;y)
$$

and generating random samples from this to approximate the posterior.

To generate the random samples, we can use **rejection sampling** or **importance sampling** for simple models. There are also many probabilistic programming tools available to automatically generate simulations.

### Grid Approximation

This method, also know as **direct discrete approximation**, works as follows:

1.  Create an even-spaced grid $g_1 = a + i/2,…,g_m = b - i/2$ where $a$ and $b$ are the lower and upper bounds of the parameter space of the posterior and $i$ and $m$ are the grid increments and number of grid points respectively.

2.  Evaluate the values of the unnormalized posterior density across the grid points $q(g_j;y)$ and normalize them by their sum to estimate the posterior values:

    $$
    \hat{p_j} = \frac{q(g_j;y)}{\sum_{i = 1}^{m}q(g_i;y)}$$

3.  For every iteration $s = 1,…,S:$

    1.  Generate a sample for $\theta_s$ from a categorical distribution with $m$ outcomes (\$g_1,...,g_m\$) with respective probabilities $\hat{p}_1,...,\hat{p}_m$

    2.  Add zero-centered uniformally distributed noise $\epsilon$ that has an interval length equal to the grid interval spacing such that:

        $$\hat{\theta}_s = \theta_s + \epsilon$$

        where $\epsilon \sim Uniform(-i/2,i/2)$

This generates a process similar to numerical integration, meaning that this approximation approach is limited to a finite interval.

Note: I have been strictly using $\theta$ as the symbol for parameters across the different models used to make it clear what symbol represents the distribution parameter. In practice, different distributions make use of different symbols for their parameter. Examples are $\alpha, \beta, \lambda$ some of which were used in previous sections when defining the prior distributions.

#### Example

We can demonstrate the usage of grid approximation with the Poisson-gamma model we explored previously. Simulation isn't necessary here since the posterior is closed-form, but this will at least demonstrate how grid approximation is able to approximate it.

Recall that the posterior of the Poisson-Gamma model had the following form:

$$
\Theta|Y \sim Gamma(\alpha + s_n,\beta + n)
$$

```{r}
true_theta <- 3
a <- b <- 1
n <- 5
set.seed(123)

y <- rpois(n,true_theta)
#Un-normalized Posterior (numerator for gamma distribution)
q <- function(theta,y,n,alpha,beta){
  theta^(alpha + sum(y) - 1) * exp(-(n + beta)*theta)
}

#Explore theta on (0,25) --> Need a fixed range 
low_bound <- 0
high_bound <- 20
#Grid increment
i <- 0.01
grid <- seq(low_bound + i/2, high_bound - i/2,by = i)
n_sim <- 10000
grid_n <- length(grid)
grid_val <- q(grid,y,n,a,b)
#Normalize to create a proper categorical distribution
grid_norm <- grid_val/sum(grid_val)

## Simulate values
sim_idx <- sample(1:grid_n,n_sim,prob = grid_norm,replace = TRUE)

#Noise parameter
e <- runif(n_sim,-i/2,i/2)
theta_sim <- grid[sim_idx] + e

par(mfrow = c(2,1),mar = c(4,4,2,2))
## Visualize with histogram
hist(theta_sim,col = 'orange',breaks = seq(0,8,0.25), probability = TRUE,
     main = 'Grid Approximation of Poisson-Gamma Model',
     xlab = expression(theta),xlim = c(0,6))
lines(grid,dgamma(grid,a + sum(y),b + n),col = 'lightblue',lwd = 4)
legend('topright', legend = 'Ground Truth', col = 'lightblue',lwd = 4)

## Visualize with density estimate
plot(grid,dgamma(grid,a + sum(y),b + n), type = 'l',col = 'lightblue',lwd = 4, ylab = 'Density',
     main = 'Grid Approximation of Poisson-Gamma Model',
     xlab = expression(theta),xlim = c(0,6))
lines(density(theta_sim),col = 'orange',lwd = 4)
legend('topright', legend = c('Estimate','Ground Truth'), col = c('lightblue','orange'),lwd = 4)
```

Note that we needed to explore $\theta$ on a fixed interval (0,25) to be able to approximate it. We can determine the plausible range of values to explore using the prior (if it is informative) or the likelihood. The key requirement is that we make sure the interval covers nearly all of the pmf for the distribution.

We can clearly see that the grid approximation has allowed us to correctly approximate the posterior without having to evaluate it analytically.

#### Example #2 : Non-conjugate priors (Incomplete)
