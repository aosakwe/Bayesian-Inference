[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Bayesian Inference",
    "section": "",
    "text": "This book contains my notes from topics on Bayesian Inference.\n\n\n\n\n\n\nWarning\n\n\n\nThis webpage is still incomplete. If you see mistakes or missing material, feel free to contact me or make a pull request!\n\n\n\n1 Chapters\n\nIntroduction\nBinomial Model\nConjugate Distributions\nPosterior Summary\nLikelihood Considerations\nOptimal Decision Making\nPosterior Approximation\nBayesian Models\nMore Models!\nNon-Parametrics\nPractice Problems"
  },
  {
    "objectID": "Main.html#premise",
    "href": "Main.html#premise",
    "title": "2  Introduction",
    "section": "2.1 Premise",
    "text": "2.1 Premise\nThis project allows me to implement different Bayesian modelling techniques I have seen through class and my own research. In addition, I hope it serves as educational material for others. I have found that the best way to learn is through practice, and so this project is expected to help me master the material I need to undertake my research projects."
  },
  {
    "objectID": "Binomial_Model.html#binomial-model",
    "href": "Binomial_Model.html#binomial-model",
    "title": "3  Binomial Model",
    "section": "3.1 Binomial Model",
    "text": "3.1 Binomial Model\nHere I will explore the binomial model, where each random variable represents a success (1) or failure (0) of a trial. The usual example is coin tossing, which, unless an unfair coin is used, always has a probability of 0.5 for landing a head or tail. It is therefore more interesting to explore a problem such as the thumbtack toss, where the ground truth probability of success (landing with the point up) is less well defined.\nWe can assume n trials are done and count the total number of trials where the point lands up (\\(s_n\\) ) to find an approximation of the ground truth for the probability of success \\(\\theta\\) . A feasible approach to estimating \\(\\theta\\) would be to divide the number of success \\(s_n\\) by the number of trials \\(n\\):\n\\[\n\\theta = \\frac{s_n}{n}\n\\]\nHowever, this estimation is only meaningful for large n and provides no estimate of the uncertainty of the measurement. Hence, the field of statistical inference becomes relevant as the latter can be achieved by creating a model for the problem.\n\n3.1.0.1 Generate Data\nTo create the model, we first generate data by tossing the thumbtack say, 25 times (\\(n = 25\\)). We can then present the problem as a binomial model where:\n\\[\ns_n \\sim Bin(n,\\theta)\n\\]\nwhich takes the form:\n\\[\nf(s_n;n,\\theta) = \\binom{n}{s_n}\\theta^{s_n}(1-\\theta)^{n-s_n}\n\\]\n\n\n\n\n\nThe above plot shows how the choice of \\(\\theta\\) affects the overall distribution. From here, we can try to infer the true value of theta using a frequentist or Bayesian approach.\n\n\n3.1.1 Maximum Likelihood\nThe frequentist approach relies on what is called the likelihood: the probability that the distribution has a certain parameter given the data we collected. From this equation we try to find the maximum likelihood. That is, we try to find the parameter that maximizes the likelihood for the data we collected. For our binomial model, we then have:\n\\[\n\\hat{\\theta}(s_n) = \\underset{\\theta}{argmax}L(\\theta;s_n)\n\\]\nwhere\n\\[\nL(\\theta;s_n)\n\\]\nis the likelihood function we are maximizing.\nFor convenience, the likelihood is often expressed as a log-likelihood which is usually easier to compute. The first step is to ignore the normalizing constant, as it does not depend on \\(\\theta\\) . We then have:\n\\[\nl(\\theta;s_n) = logL(\\theta;s_n)\n\\]\n\\[\n= logf(s_n;n,\\theta) \\propto  log(\\theta^{s_n}(1-\\theta)^{n-s_n})\n\\]\n\\[\n= s_nlog\\theta + (n-s_n)log(1-\\theta)\n\\]\nDifferentiating the equation helps us identify the extreme values:\n\\[\nl'(\\theta;s_n) = \\frac{s_n}{\\theta} - \\frac{n-s_n}{1-\\theta} = 0\n\\]\n\\[\n= s_n-s_n\\theta - n\\theta + s_n\\theta = s_n -n\\theta\n\\]\n\\[\n\\theta = \\frac{s_n}{n}\n\\]\n\n\n\n\n\nWe can also use the second derivative which is always negative, indicating that the log-likelihood is concave and that the extreme value is a global maximum.\nAlthough this approach gives us an estimate of the model parameters, there is no indication of our confidence in the result and on the likelihood of other parameter values. This is due to the frequentist approach treating \\(\\theta\\) as a constant as opposed to an unknown random variable.\nThis is circumvented by calculating the maximum likelihood estimates from all observable data sets. In this case, we are observing the random variable \\(S_n\\) as opposed to the constant \\(s_n\\) and have the following *maximum likelihood estimate:\n\\[ \\hat{\\theta}(Y) = \\frac{Y}{n}\\]\nWe can then estimate the standard error and construct confidence intervals for our parameter values. Hypothesis testing is also a possibility.\nNote: fill out an example showing how to get the above.\n\n\n3.1.2 Bayesian Model\nIn the Bayesian framework, we treat the distribution parameters as random variables. We are first interested in knowing what the probability of the thumbtack landing upwards is prior to tossing any. This prior essentially defines the distribution from which our parameter \\(\\theta\\) is sampled from. For example, we can set this to be a beta distribution:\n\\[\n\\theta  \\sim Beta(\\alpha,\\beta)\n\\]\nWe can also formulate the sampling distribution which is the distribution of the data given the parameter. It is denoted as follows:\n\\[\nf_{S_n|\\Theta}(s_n | \\theta)\n\\] This notation emphasizes that the sampling distribution is a conditional probability distribution. We therefore have a model composed of the following prior and sampling distributions: \\[\nS_n|\\theta \\sim Bin(n,\\theta)\n\\]\n\\[\n\\theta \\sim Beta(\\alpha,\\beta)\n\\] With this tools, we can now focus on updating our parameter estimates based on observed data. This step is what has us compute the so-called posterior distribution of the parameter. We start with:\n\\[\nf_{\\theta,\\S_n}(\\theta,s_n) = f_{\\theta|S_n}(\\theta|s_n)f_{S_n}(s_n)\n\\] \\[\nf_{\\theta|S_n}(\\theta|s_n) = f_{\\theta,\\S_n}(\\theta,s_n)/f_{S_n}(s_n)\n\\] \\[\nf_{\\theta|S_n}(\\theta|s_n) = \\frac{f_{\\theta}(\\theta)f_{\\S_n|\\theta}(s_n|\\theta)}{f_{S_n}(s_n)}\n\\] As the denominator on the RHS has no dependence on \\(\\theta\\), it can be ignored and the posterior can be seen as proportional to the joint distribution (the numerator):\n\\[\nf_{\\theta|S_n}(\\theta|s_n) \\propto f_{\\theta}(\\theta)f_{\\S_n|\\theta}(s_n|\\theta) = \\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)}\\theta^{\\alpha - 1}(1-\\theta)^{\\beta - 1} * \\binom{n}{s_n}\\theta^{s_n}(1-\\theta)^{n-s_n}\n\\] \\[\n\\propto \\theta^{s_n + \\alpha_0 - 1}(1-\\theta)^{n-s_n + \\beta_0 - 1}\n\\]\nThis derivation is similar in form to the beta distribution given the following, new representation of the posterior:\n\\[\n\\Theta|S_n \\sim Beta(s_n + \\alpha_0, n - s_n + \\beta_0)\n\\] We can now compare the prior and posterior:\n\nn &lt;- 25\nsn &lt;- 15\na &lt;- 2\nb &lt;- 3\na_n &lt;- a + sn\nb_n &lt;- n - sn + b\nthetas &lt;- seq(0,1,0.01)\nprior &lt;- dbeta(thetas,a,b)\nposterior &lt;- dbeta(thetas,a_n,b_n)\nplot(thetas,prior, xlab = 'Theta Value',ylab = 'f(Theta)',\n     col = 'blue',ylim = c(0,max(posterior)),\n     type = 'l')\nlines(thetas,posterior, col = 'red')\nlegend('topleft',inset = 0.02,legend = c('Beta(a,b)','Beta(a_n,b_n)'),\n       col = c('blue','red'),\n       lwd = 2)\n\n\n\n\nThis reveals that the true value of theta (given the observed trials) is most likely between ~ 0.4 and 0.8.\n\npbeta(0.8,a_n,b_n) - pbeta(0.4,a_n,b_n) \n\n[1] 0.9649619\n\n\nWe can also calculate the posterior mean by finding the mean of the beta distribution we derived for the posterior\n\\[\nE(\\theta|S_n = s_n) = \\frac{\\alpha_n}{\\alpha_n + \\beta_n} = \\frac{s_n + \\alpha_0}{\\alpha_0 + n + \\beta_0} = \\frac{17}{30}\n\\]\n\n\n3.1.3 Components of Bayesian Inference\nWe can now generalize the example to highlight the key concepts for Bayesian inference.\nWe work on a problem with observed data coming from n observations which we can then use to create and update a model representing the generative process behind the data. Here, we will focus on parametric inference, where the observed data helps us better estimate the model parameters.\n\n3.1.3.1 Sampling Distribution and the Likelihood function\nThe conditional distribution of the data given the parameter values, \\[f_{S_n|\\Theta}(s_n|\\theta)\\] , can be referred to as the sampling distribution or the likelihood function. The be exact, the sampling distribution presents the above as a function of the observed data, whereas the likelihood presents it as a function of the model parameters.\nIf we assume that our data is independent, given \\[\\theta\\], we can present the joint sampling distribution as a product of the sampling distributions for each random variable or trial:\n\\[\nf_{S_N|\\Theta}(s|\\theta) = \\prod_{i =1}^{N}f_{S_i|\\Theta}(s_i|\\theta)\n\\]\nWhich becomes even simpler if the observations ar i.i.d (not only independent, but coming from the same distribution):\n\\[\nf_{S_N|\\Theta}(s|\\theta) = \\prod_{i =1}^{N}f(s_i|\\theta)\n\\]\nMany different models can be used for the sampling distribution, but it is usually recommended to use models whose behavior best describes the observed phenomena (e.g. our thumbtack example is a clear success or failure scenario, which makes the binomial distribution suitable).\n\n\n3.1.3.2 Prior distribution\nThe prior distribution represents the distribution from which the parameter values of the sampling distribution are generated. It represents our beliefs in the parameter values prior to observing any data.\nIn situations where there is little confidence in the possible values for the parameter, it is best to just a vague prior distribution to reduce its influence on the model and put a greater emphasis on the observed data (uninformative prior). We could also have an informative prior which has a greater influence on the model which can enforce sparsity on values we believe should be zero. We can also call the prior distribution a parametric distribution, which depends on hyperparameters (in the thumbtack case, these would be \\(\\alpha\\) and \\(\\beta\\) ).\nPosterior distribution\nFor the posterior, we make use of Bayes’ theorem to formulate it. The normalizing constant, which has no dependence on the parameter can be treated as a constant w.r.t. the parameter values and is often ignored. We can therefore compute the posterior as being proportional to the joint distribution.\n\n\n3.1.3.3 Marginal Likelihood\nThe normalizing constant is also described as the marginal likelihood, as it represents the joint probability of the observed data after marginalizing out the parameter $\\theta$. This presents itself as an integral in continuous cases and as a sum in discrete cases (for the parameter). This can also be seen as the expectation of the sampling distribution, where we average the values over the full sample space for \\(\\theta\\) .\n\n\n\n3.1.4 Prediction\nBeyond estimating the true parameter values, we are also interested in predicting the unobserved data (n + m). The simplest way to achieve this would be to use the MLE for the model parameters and predict the new data. This however, can lead to estimates that are biased if the observed data used for the MLE came from a small sample size (e.g.: only three tosses and all fell down). We can instead go the Bayesian route and compute a probability for the new observations given the observed data\n\\[\nf_{\\tilde{S}|S}(\\tilde{s}|s)\n\\]\nThe parameter values aren’t present in the function, but it is important to realize that they are need to derive the predictive distribution. In essence, the posterior predictive distribution is calculated as the product of the sampling distribution for the new data and the posterior distribution from the observed data, which acts as un updated prior distribution. In essence, the same way we updated \\(\\alpha\\) and \\(\\beta\\) for the observed data, we do it again by updating their updated values.\n\\[\nf_{\\tilde{S}|S_n}(\\tilde{s}|s_n) = \\int_{\\Theta}\nf_{\\tilde{S}|\\Theta}(\\tilde{s}|\\theta)f_{\\theta|S_n}(\\theta|s_n)\nd\\theta\n\\]\n\\[\nf_{\\tilde{S}|S_n}(\\tilde{s}|s_n) = \\int_{\\Theta}\\binom{m}{\\tilde{s}}\n\\theta^{\\tilde{s}}(1-\\theta)^{m - \\tilde{s}}\n\\frac{1}{B(\\alpha_n,\\beta_n)}\n\\theta^{\\alpha_n - 1}(1-\\theta)^{\\beta_n - 1}d\\theta\n\\]\nRemoving constants and recognizing the new formulation of a beta function, as in the earlier example, allows us to define the new posterior predictive function given the updated prior:\n\\[\nf_{\\tilde{S}|S_n}(\\tilde{s}|s_n) = \\binom{m}{\\tilde{s}}\\frac{B(\\tilde{s} + \\alpha_n,m + \\beta_n - \\tilde{s})}{B(\\alpha_n,\\beta_n)}\n\\]\nWhich represents the beta-binomial distribution. With this formulation we can therefore iteratively update our prior distribution as we collect more samples, leading to a progressively more accurate posterior predictive distribution."
  },
  {
    "objectID": "Conjugate_Dist.html",
    "href": "Conjugate_Dist.html",
    "title": "4  Conjugate Distributions",
    "section": "",
    "text": "A conjugate distribution refers to a pair of sampling and prior distributions where the posterior distribution is of the same family as the prior distribution. A family of distributions implies a set of distributions which have a similar form and only differ in the values of their parameters.\nConjugate distributions play a big role in Bayesian analyses as they greatly simplify model formulations. After selecting a plausible sampling distribution, selecting a conjugate prior ensures that the posterior distribution is easy to derive and is tractable. The only other way for the posterior to be tractable is if the explored parameter space is finite AND discrete. Posteriors that do not match these two cases have to be approximated (either by sampling processes or variational inference).\nIn the previous document, we saw an example of a conjugate pair: the beta and binomial distributions.\n\n4.0.1 One-parameter conjugate models\n\n4.0.1.1 Poisson-gamma model\nA Poisson distribution is a discrete distribution which can take all positive integer values. It is often used to model counts, such as points in a sport or the number of daily customers in a store. The distribution has the following properties:\n\\[\n\\mathbb{E}[Y] = Var[Y] = \\theta\n\\]\n\nn &lt;- 100\ntheta &lt;- 5\nset.seed(123)\nmean_estimate &lt;- c()\nvar_estimate &lt;- c()\nfor (i in 1:n){\n  y &lt;- rpois(i,theta)\n  mean_estimate &lt;- c(mean_estimate,mean(y))\nvar_estimate &lt;- c(var_estimate,var(y))\n}\npar(mfrow = c(2,1))\nplot(1:n,mean_estimate,col = 'red',type = 'l',xlab = 'Sample Size',\n     ylab = 'Sample Mean')\nlines(1:n,rep(theta,n),col = 'black')\nplot(1:n,var_estimate,col = 'blue',type = \"l\",xlab = 'Sample Size',\n     ylab = 'Sample Variance')\nlines(1:n,rep(theta,n),col = 'black')\n\n\n\n\nAs we can see, with sufficient sample size, the mean and variance are both similar to the \\(\\theta\\) used to generate the counts.\nNow, we can make use of Bayesian analysis to infer this theta using the data. If we assume that each count is independently and identically distributed (which they are), we can model then as Poisson random variables.\nSince the rate parameter of the Poisson can take on any positive real number, the parameter should be sampled from a distribution that covers a similar range. In this case (and given that we know it is a conjugate prior for the Poisson distribution), we can use the Gamma distribution where\n\\[\n\\theta \\sim Gamma(\\alpha,\\beta)\n\\]\nThis ensures that \\(\\theta\\) is sampled from a distribution which covers all non-negative real numbers and that allows for the derivation of a closed-form prior. We can now estimate the posterior for \\(\\theta\\) :\n\\[\np(\\theta|Y) \\approx p(Y|\\theta)\\pi_0(\\theta)\n\\]\n\\[\np(Y|\\theta) = \\prod_{i = 1}^n\\theta^{y_i}\\frac{e^{-\\theta}}{y_i!} \\propto\n\\theta^{s_n}{e^{-n\\theta}}\n\\]\nwhere \\(s_n = \\sum_{i = 1}^{n}y_i\\) .\n\\[\np(\\theta|Y) \\propto \\theta^{s_n}{e^{-n\\theta}}\\theta^{\\alpha-1}e^{-\\beta\\theta} = \\theta^{\\alpha + s_n -1}e^{-(\\beta + n)\\theta}\n\\]\nWe notice that the current formulation is similar to the gamma distribution with parameters \\(\\alpha_n = \\alpha + s_n\\) and \\(\\beta_n = \\beta + n\\) . Therefore, the posterior is:\n\\[\n\\Theta|Y \\sim Gamma(\\alpha + s_n,\\beta + n )\n\\]\n\na &lt;- 1\nb &lt;- 2\ntheta &lt;- seq(0,10,length = 100)\npar(mfrow = c(3,2),mar = c(5, 5, 1.5, 1.5))\nfor (i in c(5,10,25,50,75,100)){\n  prior &lt;- dgamma(theta,shape = a, rate = b)\n  posterior &lt;- dgamma(theta,shape = a + sum(y[1:i]), rate = b + i)\n  plot(theta,prior, type = 'l', col = 'orange', xlab = 'Theta Values',\n       ylab = 'Density',\n       main = paste('n = ',i,sep = ''))\n  lines(theta,posterior,col = 'green')\n  abline(v = 5, lty = 2)\n  legend('topright', inset = .02, legend = c('Prior', 'Posterior'),\n         col = c('orange', 'green'), lwd = 2)\n}\n\n\n\n\nWe can see that as the sample size increases, the posterior density peaks near the true \\(\\theta\\) value.\n\n\n4.0.1.2 Prediction with Poisson-Gamma Model\nAs seen in the last document, we can the compute a posterior predictive distribution to predict the probability of the next unobserved value given the observed counts. For the sake of brevity, I will not cover it here (but all the material required is present in the beta-binomial example!).\n\n\n\n4.0.2 Prior Distributions\nAs seen in the derived posteriors, the choice of the prior not only affects our derivation of the posterior, but can also influence the posterior probability. This is particularly true when the sample size is small. It is therefore imperative to select an appropriate prior that correctly guides the posterior distribution given what we know about the data. This is known as an informative prior which will help make the solution more stable and accurate at small sample sizes by helping restrict the sampling of the parameters for the sampling distribution to a range of plausible values. For the model described above, the gamma is a suitable prior as it ensures that the \\(\\theta\\) parameter sampled for use in the Poisson distribution is always non-negative, as is required.\nThere are many cases where we may not know enough about the data-generating process to make reliable assumptions on the prior’s form. In this case, we would prefer to use a non-informative prior B which will not influence the downstream posterior distribution. A logical option here would be the uniform distribution which sets an equal probability for all values within a range. This however can lead to two key issues:\n\nUniform is limited to a finite range of values. If the true parameter estimate can be any real number, we would not be able to use a proper distribution and provide each value with an equal probability. This can be resolved by using a constant for each value on a non-finite range, but the use of an improper distribution is not always desirable.\nAlthough the uniform distribution is non-informative, it is possible that a re-parameterization of the sampled parameters can become informative, making it impractical for certain use cases.\n\nAs a result, a novel form of non-informative prior can be used, which is robust to re-parameterization - The Jeffrey’s Prior. The key advantage of this prior is that it applies to an infinite domain, and remains un-informative after changing the parameterization. We can look at the concept below.\nLet’s assume that we are modeling observations \\(Y_i \\sim Exponential(\\theta)\\) and use \\(Gamma(\\alpha,\\beta)\\) as a prior. Skipping the derivation, we have the following equation for the posterior of \\(\\theta\\) :\n\\[\n\\Theta|Y \\sim Gamma(\\alpha + n,\\beta + s_n)\\]\nWhere \\(s_n = \\sum_{i =1}^ny_i\\). We can see that for a small \\(n\\), the posterior distribution will closely resemble the prior.\n\na &lt;- 2\nb &lt;- 3\ntheta &lt;- sample(1:10,1)\nn &lt;- 5\ny &lt;- rexp(n,theta)\nsn &lt;- sum(y)\nbign &lt;- 250\nbig_sn &lt;- sum(rexp(bign,theta))\ntheta_seq &lt;- seq(0,10,by = 0.01)\nplot(theta_seq,dgamma(theta_seq,a + n,b + sn),type = 'l',col = 'red',\n     xlab = expression(theta),\n     ylab = 'Density',lwd = 2,ylim = c(0,5))\nlines(theta_seq,dgamma(theta_seq,a + bign,b + big_sn), col = 'orange',lwd = 2)\nlines(theta_seq,dgamma(theta_seq,a,b), col = 'blue',lwd = 2)\nabline(v = theta,lwd = 2)\nlegend('topright', legend = c('Small N Post.','Big N Post.','Prior','True Theta'), col = c('red','orange','blue','black'),lwd = 2)\n\n\n\n\nAs we can see, the choice of the prior will influence the posterior until enough samples have been collected. If the prior distribution is not chosen correctly (as is the case here), we can see that a large number of samples are needed before we converge to the true estimate.\nThe non-informative prior however, will help us avoid this and entirely drive the posterior distribution by the observations with no risk of selecting a misleading prior.\n\n4.0.2.1 Jeffrey’s Prior\nThe key issue with the uniform distribution as a prior is that in the case of reparemeterization of the target parameter \\(\\theta\\) such that \\(\\phi = g(\\theta)\\) we have that\n\\[\\pi_{0,\\theta}(\\theta) = c  \\rightarrow \\pi_{0,\\phi}(\\phi) = c  \\times J(\\theta \\rightarrow \\phi) \\]\nWhere \\(J(\\theta \\rightarrow \\phi)\\) is the Jacobian of the transformation. This implies that there is no guarantee that reparameterizing the uniform prior will retain the non-informative behavior. The jeffrey’s prior is defined using the following structure for the prior:\n\\[\n\\pi_0(\\theta) \\propto  |\\mathcal{I}_\\theta(\\theta)|^{\\frac{1}{2}}\n\\]\nWhere \\(\\mathcal{I}_\\theta(\\theta)\\) is the Fisher Information of the pdf. The Fisher Information can be represented as the negative expectation of the second derivative of the log-pdf. In practice, for k parameters \\((\\theta_1,…,\\theta_k)\\) , the fisher Information will take the form of a \\(k \\times k\\) matrix (REVIEW). In practice, we can see that any transformation of \\(\\theta\\) will give the same value for the Jeffrey’s prior.\nWe can then apply this to our example with the Exponential pdf.\n\\[ \\mathcal{I}_\\theta(\\theta) = -\\mathbb{E}_Y[\\frac{\\partial^2}{\\partial^2\\theta_j\\theta_l}log(f_Y(y;\\theta))]  \\]\n\\[ log(f_Y(y;\\theta)) = log(\\theta e^{-\\theta y}) = log(\\theta) -\\theta y \\]\n\\[ \\frac{\\partial}{\\partial\\theta_j}log(f_Y(y;\\theta)) = \\frac{1}{\\theta} - y \\]\n\\[ \\frac{\\partial^2}{\\partial^2\\theta_j\\theta_l}log(f_Y(y;\\theta)) = -\\frac{1} {\\theta^2} \\]\n\\[ \\mathcal{I}_\\theta(\\theta) = -\\mathbb{E}_Y[{-\\frac{1}{\\theta^2}}] \\]\nSince \\(\\theta\\) is constant with respect to Y, we get\n\\[ \\mathcal{I}_\\theta(\\theta) = \\frac{1}{\\theta^2} \\]\nTherefore,\n\\[ \\pi_0(\\theta) \\propto  |\\mathcal{I}_\\theta(\\theta)|^{\\frac{1}{2}} = \\frac{1}{\\theta} \\]\n\\[ \\pi_n(\\theta) \\propto {\\theta^n e^{-\\theta s_n}} \\theta^{-1} =  {\\theta^{n-1} e^{-\\theta s_n}} \\]\nThis form matches the gamma pdf with \\(\\alpha_n = n\\) and \\(\\beta_n = s_n\\). Leading to the following posterior:\n\\[\n\\Theta|Y \\sim Gamma(n,s_n)\n\\]\nWhich depends entirely on the observed data. We can see how this affects our modeling below.\n\nn &lt;- 25\ny &lt;- rexp(n,theta)\nsn &lt;- sum(y)\ntheta_seq &lt;- seq(0,10,by = 0.01)\nplot(theta_seq,dgamma(theta_seq,n,sn),type = 'l',col = 'red',\n     xlab = expression(theta),\n     ylab = 'Density',lwd = 2,ylim = c(0,2))\nlines(theta_seq,dgamma(theta_seq,a + n,b + sn), col = 'orange',lwd = 2)\nabline(v = theta,lwd = 2)\nlegend('topright', legend = c('Jeffrey-Exponential','Gamma-Exponential','True Theta'), col = c('red','orange','black'),lwd = 2)\n\n\n\n\nWe can see that at a smaller sample size, the jeffrey’s prior-based model is much closer to the true value than the misleading prior. We therefore see the advantage of using a non-informative prior when we are not confident in our choice of a prior. However, it is important to note that this will not outperform a well chosen prior."
  },
  {
    "objectID": "PosteriorSummary.html",
    "href": "PosteriorSummary.html",
    "title": "5  Studying the Posterior",
    "section": "",
    "text": "TODO - make a function for the 6-panel CI plots\nBeyond defining the posterior distribution, it is important to find a way to describe the information it provides on the plausible parameter values. The easiest way is to plot the distribution (if we have 3 or less parameter values). Alternatively, we can simulate observations from the distribution and visualize them.\nThere are also ways to summarize the information provided by the posterior numerically. Examples of such metrics are the usual descriptive statistics such as the mean, variance, mode etc.\n\n5.0.1 Credible Intervals\nThe credible interval acts as a Bayesian version of the frequentist confidence interval. However, credible intervals provide a much more intuitive interpretation as a 95% credible interval contains the true parameter value with 95% probability. On the other hand the frequentist confidence interval says that 95% of the time, the true parameter value falls within the designated range.\n\n5.0.1.1 Definition\nIn the case of a one-dimensional parameter \\(\\Theta \\in \\Omega\\) with a confidence level of \\(\\alpha \\in (0,1)\\) there exists an interval \\(I_\\alpha\\) which contains a proportion \\(1 - \\alpha\\) of the pmf of the posterior:\n\\[\nP(\\Theta \\in I_\\alpha|Y = y) = 1- \\alpha\n\\]\nwhich is referred to as the credible interval. We usually assess the 95% credible interval. The region of parameter values that contain the credible interval is called the credible region. Applying this concept to the prior can be a useful way of selecting an informative prior, as we can find a distribution whose credible interval complements the interval found with an estimate from the observed data.\n\n\n\n5.0.2 Equal-tailed and One-tailed Intervals\nAn equal-tailed interval aims at finding the credible interval between the \\(\\alpha/2\\) and the \\(1-\\alpha/2\\) quantiles. Most credible intervals are calculated as equal-tailed. However, this is only sensible if the posterior is unimodal and symmetric. In other cases, a one-tailed (\\([0,1-\\alpha]\\) or \\([\\alpha,1]\\)) may be more reasonable.\n\n\n5.0.3 Examples\nWe can explore the concept using the gamma-poisson model from the previous section.\n\n5.0.3.1 Equal-tailed\n\nn &lt;- 100\ntrue_theta &lt;- 5\nset.seed(123)\ny &lt;- rpois(n,true_theta)\na &lt;- 1\nb &lt;- 2\nconf &lt;- 0.05\ntheta &lt;- seq(0,10,length = 100)\npar(mfrow = c(3,2),mar = c(5, 5, 1.5, 1.5))\nq_low &lt;- qgamma(conf,a,b)\nq_high &lt;- qgamma(1-conf,a,b)\nprior &lt;- dgamma(theta,shape = a, rate = b)\nplot(theta,prior, type = 'l', col = 'orange', xlab = 'Theta Values',\n       ylab = 'Density',\n       main = \"Prior\")\n  abline(v = 5, lty = 2)\npolygon(c(q_low,theta[theta &gt;= q_low & theta &lt;= q_high],q_high),\n        c(0,prior[theta &gt;= q_low & theta &lt;= q_high],0),\n        col = 'orange',border = 'orange')\nlegend('topright', inset = .02, legend = c('Prior'),\n         col = c('orange'), lwd = 2)\nfor (i in c(5,10,25,50,100)){\n  prior &lt;- dgamma(theta,shape = a, rate = b)\n  posterior &lt;- dgamma(theta,shape = a + sum(y[1:i]), rate = b + i)\n  plot(theta,prior, type = 'l', col = 'orange', xlab = 'Theta Values',\n       ylab = 'Density',\n       main = paste('n = ',i,sep = ''))\n  lines(theta,posterior,col = 'green')\n  q_low &lt;- qgamma(conf,a + sum(y[1:i]),b + i)\n  q_high &lt;- qgamma(1-conf,a + sum(y[1:i]),b + i)\n  polygon(c(q_low,theta[theta &gt;= q_low & theta &lt;= q_high],q_high),\n        c(0,posterior[theta &gt;= q_low & theta &lt;= q_high],0),\n        col = 'lightgreen',border = 'lightgreen')\n  abline(v = 5, lty = 2)\n  legend('topright', inset = .02, legend = c('Prior', 'Posterior'),\n         col = c('orange', 'green'), lwd = 2)\n}\n\n\n\n\n\n\n5.0.3.2 One-tailed\n\nn &lt;- 100\ntrue_theta &lt;- 5\nset.seed(123)\ny &lt;- rpois(n,true_theta)\na &lt;- 1\nb &lt;- 2\nconf &lt;- 0.05\ntheta &lt;- seq(0,10,length = 100)\npar(mfrow = c(3,2),mar = c(5, 5, 1.5, 1.5))\nq_low &lt;- 0\nq_high &lt;- qgamma(1-conf,a,b)\nprior &lt;- dgamma(theta,shape = a, rate = b)\nplot(theta,prior, type = 'l', col = 'orange', xlab = 'Theta Values',\n       ylab = 'Density',\n       main = paste('n = ',i,sep = ''))\n  abline(v = 5, lty = 2)\npolygon(c(q_low,theta[theta &gt;= q_low & theta &lt;= q_high],q_high),\n        c(0,prior[theta &gt;= q_low & theta &lt;= q_high],0),\n        col = 'orange',border = 'orange')\nlegend('topright', inset = .02, legend = c('Prior'),\n         col = c('orange'), lwd = 2)\nfor (i in c(5,10,25,50,100)){\n  prior &lt;- dgamma(theta,shape = a, rate = b)\n  posterior &lt;- dgamma(theta,shape = a + sum(y[1:i]), rate = b + i)\n  plot(theta,prior, type = 'l', col = 'orange', xlab = 'Theta Values',\n       ylab = 'Density',\n       main = paste('n = ',i,sep = ''))\n  lines(theta,posterior,col = 'green')\n  q_high &lt;- qgamma(1-conf,a + sum(y[1:i]),b + i)\n  polygon(c(q_low,theta[theta &gt;= q_low & theta &lt;= q_high],q_high),\n        c(0,posterior[theta &gt;= q_low & theta &lt;= q_high],0),\n        col = 'lightgreen',border = 'lightgreen')\n  abline(v = 5, lty = 2)\n  legend('topright', inset = .02, legend = c('Prior', 'Posterior'),\n         col = c('orange', 'green'), lwd = 2)\n}\n\n\n\n\n\n\n\n5.0.4 Influence of the Prior\nWe can use the prior distribution to demonstrate how its influence on the posterior changes as more data is observed. The credible interval of the prior tells us the 95% of the probability mass should lie before any data is observed (i.e. the range of values we believe to be most likely). We repeat the previous even-tailed example using larger parameter values for the prior to emphasize its influence more clearly.\n\nn &lt;- 500\ntrue_theta &lt;- 5\nset.seed(123)\ny &lt;- rpois(n,true_theta)\na &lt;- 10\nb &lt;- 20\nconf &lt;- 0.05\ntheta &lt;- seq(0,10,length = 100)\npar(mfrow = c(3,2),mar = c(5, 5, 1.5, 1.5))\nq_low &lt;- 0\nq_high &lt;- qgamma(1-conf,a,b)\nprior &lt;- dgamma(theta,shape = a, rate = b)\nplot(theta,prior, type = 'l', col = 'orange', xlab = 'Theta Values',\n       ylab = 'Density',\n       main = paste('n = ',i,sep = ''))\n  abline(v = 5, lty = 2)\npolygon(c(q_low,theta[theta &gt;= q_low & theta &lt;= q_high],q_high),\n        c(0,prior[theta &gt;= q_low & theta &lt;= q_high],0),\n        col = 'orange',border = 'orange')\nlegend('topright', inset = .02, legend = c('Prior'),\n         col = c('orange'), lwd = 2)\nfor (i in c(5,50,100,250,500)){\n  prior &lt;- dgamma(theta,shape = a, rate = b)\n  posterior &lt;- dgamma(theta,shape = a + sum(y[1:i]), rate = b + i)\n  plot(theta,prior, type = 'l', col = 'orange', xlab = 'Theta Values',\n       ylab = 'Density',\n       main = paste('n = ',i,sep = ''))\n  lines(theta,posterior,col = 'green')\n  q_high &lt;- qgamma(1-conf,a + sum(y[1:i]),b + i)\n  polygon(c(q_low,theta[theta &gt;= q_low & theta &lt;= q_high],q_high),\n        c(0,posterior[theta &gt;= q_low & theta &lt;= q_high],0),\n        col = 'lightgreen',border = 'lightgreen')\n  abline(v = 5, lty = 2)\n  legend('topright', inset = .02, legend = c('Prior', 'Posterior'),\n         col = c('orange', 'green'), lwd = 2)\n}\n\n\n\n\nThis example shows more clearly how the addition of more samples shifts the influence on the posterior from the prior to the observed data. This is why the choice of a non-informative prior can be very useful if we are uncertain of a plausible informative prior.\n\n\n5.0.5 Highest Posterior Density\nIn addition to the credible interval, we can also identify the highest posterior density (HPD) region. The HPD is a confidence region \\(I_\\alpha\\) in which the posterior density for every point within the set is greater than the posterior density for any point outside of the interval.\n\\[\nf_{\\Theta|Y}(\\theta|y) \\geq f_{\\Theta|Y}(\\theta'|y)\n\\]\nwhere \\(\\theta \\in I_\\alpha\\) and \\(\\theta' \\notin I_\\alpha\\) . It is therefore the smallest possible credible region in the distribution. As a result, the HPD region is not necessarily an interval, and can be a union of distinct intervals on the distribution. This makes it useful for multimodal posteriors where an even-tailed or one-tailed credible interval would not be able to fully represent the information provided. We can demonstrate its value with a bimodal posterior derived from a mixture of beta distributions.\n\nconf &lt;- 0.05\na1 &lt;- 10\nb1 &lt;- 20\na2 &lt;- 20\nb2 &lt;- 5\n\nbeta_mixture &lt;- function(x,a1,a2,b1,b2){\n  0.5*dbeta(x,a1,b1) + 0.5*dbeta(x,a2,b2)\n}\n#Generate Data\nn &lt;- 100000\ntheta1 &lt;- rbeta(n/2,a1,b1)\ntheta2 &lt;- rbeta(n/2,a2,b2)\ntheta &lt;- sort(c(theta1,theta2))\nq_low &lt;- theta[round(conf*n/2)]\nq_high &lt;- theta[round((1-conf/2)*n)]\nx &lt;- seq(0,1,length = 1000)\ny &lt;- beta_mixture(x,a1,a2,b1,b2)\nplot(x,y,type = 'l', col = 'blue',lwd = 2,\n     xlab = expression(theta), ylab = 'Density',\n     main = \"Credible Interval on Bimodal Distribution\")\npolygon(c(q_low,x[x &gt;= q_low & x &lt;= q_high],q_high),\n        c(0,y[x &gt;= q_low & x &lt;= q_high],0),\n        col = 'lightblue',lwd = 2, border = 'blue')\n\n\n\n\nAs described, the plot shows how the credible interval ends up including a region of the distribution with low densities and omitting other regions with higher densities. The HPD region can therefore resolve this issue:\n\ndensities &lt;- density(theta)\nhpd &lt;- HDInterval::hdi(densities,allowSplit = TRUE)\nheight &lt;- attr(hpd, 'height')\nq_low1 &lt;- hpd[1,1]\nq_high1 &lt;- hpd[1,2]\nq_low2 &lt;- hpd[2,1]\nq_high2 &lt;- hpd[2,2]\nx &lt;- seq(0,1,length = 1000)\ny &lt;- beta_mixture(x,a1,a2,b1,b2)\nplot(x,y,type = 'l', col = 'blue',lwd = 2,\n     xlab = expression(theta), ylab = 'Density',\n     main = \"HPD Region for a Bimodal Distribution\")\npolygon(c(q_low1,x[x &gt;= q_low1 & x &lt;= q_high1],q_high1),\n        c(0,y[x &gt;= q_low1 & x &lt;= q_high1],0),\n        col = 'lightblue',lwd = 2, border = 'blue')\npolygon(c(q_low2,x[x &gt;= q_low2 & x &lt;= q_high2],q_high2),\n        c(0,y[x &gt;= q_low2 & x &lt;= q_high2],0),\n        col = 'lightblue',lwd = 2, border = 'blue')\n\n\n\n\nHere we can clearly see the advantage HPD regions provide for summarizing multimodal distributions. It also demonstrates why it is important to visualize the posterior distribution before summarizing it in terms of one-dimensional summary statistics such as a mean or median.\n\n\n5.0.6 Posterior Mean\nThe mean of the posterior distribution is referred to as a Bayes Estimator:\n\\[\n\\hat{\\lambda}_{Bayes}(Y) = \\mathbb{E}[\\theta|Y]\n\\]\nThe mean for the gamma distribution is \\(\\frac{\\alpha}{\\beta}\\) so the Bayes estimator for the Poisson-gamma model derived in the last chapter is\n\\[\n\\mathbb{E}[\\theta|Y = y] = \\frac{\\alpha + s_n}{\\beta + n}\n\\]\nThe posterior mean can also be expressed as a convex combination of the mean of the prior distribution\n\\[\n\\mathbb{E}[\\theta|Y = y] = \\frac{\\alpha + s_n}{\\beta + n} = k\\frac{\\alpha}{\\beta} + (1-k)\\frac{s_n}{n}\n\\]\nWhere \\(k\\) is \\(\\frac{\\beta}{\\beta + n}\\) and shows how the posterior mean is progressively influence by the sample mean as the sample size increases. In this case, as the sample size approaches infinity, the Bayes estimator takes the form of the maximum likelihood estimator which is the sample mean for the model. This formulation also explains how the parameterization of the prior distribution affects its influence on the posterior."
  },
  {
    "objectID": "LikelihoodConsiderations.html#note-this-section-needs-work",
    "href": "LikelihoodConsiderations.html#note-this-section-needs-work",
    "title": "6  Likelihood Considerations",
    "section": "6.1 Note: this section needs work!",
    "text": "6.1 Note: this section needs work!\nWe previously looked at how the prior can influence the posterior distribution when few observations are made. To this end, as \\(n\\) increases, we expect the likelihood to play a crucial role in defining the posterior. This can be highlighted by expression the posterior on a log scale:\n\\[\nlog\\pi_n(\\theta) = \\sum_{i=1}^nlogf_Y(y_i;\\theta) + log\\pi_0(\\theta) + C\n\\]\nwhere the log-prior is constant with respect to \\(\\theta\\) whereas the influence of the log-likelihood will grow with an increasing sample size. This makes it imperative to explore the properties of the likelihood as \\(n\\) increases.\n\n6.1.1 Asymptotic Theory of the Likelihood\nThe true model represents the situation where the data \\(y\\) are realizations of iid random variables \\(Y_1,…,Y_n\\) that are drawn from a distribution with pdf \\(f_0(y)\\). In practice we aim to approximate this model by the use of a working model, where we represent the data with a parametric pdf \\(f_Y(y;\\theta)\\) where \\(\\theta\\) represents a d-dimensional parameter.\nOur analysis assumes that for a given \\(\\theta_0\\)\n\\[\nf_0(y) \\equiv f_Y(y;\\theta_0)\n\\]\nsuch that the model is correctly specified. In any case where there is \\(f_0(y) \\neq f_Y(y;\\theta)\\) for any \\(\\theta\\) , then the model is incorrectly specified. These are cases where the theory behind the model must be reconsidered.\nInterpreting \\(\\theta_0\\) in the working model:\nWe define the true value of \\(\\theta_0\\) as\n\\[\n\\theta_0 = \\underset{\\theta}{argmin}KL(f_0,f_Y(y;\\theta))\n\\tag{6.1}\\]\nRecall that\n\\[\nKL(f_0,f_Y(y;\\theta)) = \\int logf_0(y)f_0(y)dy - \\int logf_Y(y;\\theta)f_0(y)dy\n\\]\nAs our goal is to minimize KL and since \\(logf_Y(y;\\theta) = l(y;\\theta)\\), we can also express \\(\\theta_0\\) as\n\\[\n\\theta_0 = \\underset{\\theta}{argmax}\\mathbb{E}_{f_0}[l(Y;\\theta)]\n\\]\nMaximum Likelihood\nWe can maximize the expectation (mean) of the observed samples to acquire an estimator. The estimator for the maximization problem described above would be\n\\[\n\\hat{\\theta}_n = \\underset{\\theta}{argmax}\\frac{1}{n}\\sum_{i = 1}^nl(Y_i;\\theta)\n\\]\nand thanks to the weak law of large numbers:\n\\[\n\\underset{n \\to\\infty}{lim}\\frac{1}{n}\\sum_{i = 1}^nl(Y_i;\\theta) = \\mathbb{E}_{f_0}[l(Y;\\theta)]\n\\]\nIf the expectation exists. If we assume that the log density we described is at least three times differentiable wrt \\(\\theta\\), the estimate is defined as the solution to the score equations, a system of d equations given by\n\\[\n\\frac{\\partial}{\\partial\\theta}\\{\\frac{1}{n}\\sum_{i = 1}^nl(y_i;\\theta)\\} = O_d\n\\]\nOr\n\\[\n\\frac{1}{n}\\sum_{i = 1}^n\\frac{\\partial}{\\partial\\theta}\\{l(Y_i;\\theta)\\} = \\frac{1}{n}\\sum_{i = 1}^nS(y_i;\\theta) = O_d\n\\]\nWe can denote the order of the derivative with ‘.’\nTaylor Expansion\nConsider the Taylor expansion of the log-likelihood with respect to \\(\\theta\\) around \\(\\theta_0\\)\n\\[\nl(y;\\theta) = l(y;\\theta_0) + \\dot{l}(y;\\theta_0)^T(\\theta - \\theta_0) + 0.5(\\theta -\\theta_0)^T\\ddot{l}(y;\\theta_0)(\\theta - \\theta_0) + R_3(y;\\theta^*)\n\\]\nNote that \\(\\ddot{l}(y;\\theta_0)\\) is (dxd) and \\(R_3\\) is a remainder term for \\(\\theta^*\\) such that \\(|| \\theta_0 - \\theta^*|| \\leq ||\\theta_0 - \\theta||\\)\nWe can evaluate the above for all observations and sum the result\n\\[\nl_n(\\theta) = l_n(\\theta_0) + \\dot{l_n}(\\theta_0)^T(\\theta - \\theta_0) + 0.5(\\theta -\\theta_0)^T\\ddot{l_n}(\\theta_0)(\\theta - \\theta_0) + R_3\n\\]\nAt \\(\\theta = \\hat{\\theta}_n\\) and rearranging we have that\n\\[l_n(\\hat{\\theta}_n) = l_n(\\theta_0) + \\dot{l_n}(\\theta_0)^T(\\hat{\\theta}_n - \\theta_0) + 0.5(\\hat{\\theta}_n -\\theta_0)^T\\ddot{l_n}(\\theta_0)(\\hat{\\theta}_n - \\theta_0) + R_3\\] Asymptotic behavior\nWe can express the above in terms of random variables, where \\(\\hat{\\theta}_n = \\hat{\\theta}_n(Y_{1:n})\\)\n\\[l_n(\\hat{\\theta}_n) - l_n(\\theta_0) = \\dot{l_n}(\\theta_0)^T(\\hat{\\theta}_n - \\theta_0) + 0.5(\\hat{\\theta}_n -\\theta_0)^T\\ddot{l_n}(\\theta_0)(\\hat{\\theta}_n - \\theta_0) + R_3\\]\nWe can then consider for an arbitrary \\(\\theta\\)\n\\[\n\\frac{1}{n}(l_n(\\hat{\\theta}_n) - l_n(\\theta_0)) = \\frac{1}{n}\\sum_{i = 1}^{n}(l(Y_i;\\theta)-l(Y_i;\\theta_0))\n\\]\nThe RHS expression can be rewritten in terms that involve the true density\n\\[\n\\frac{1}{n}\\sum_{i = 1}^{n}(l(Y_i;\\theta)-l(Y_i;\\theta_0)) = \\frac{1}{n}\\sum_{i = 1}^{n}(l(Y_i;\\theta)-l_0(Y_i)) - \\frac{1}{n}\\sum_{i = 1}^{n}(l(Y_i;\\theta_0)-l_0(Y_i))\n\\]\nFor any \\(\\theta\\) as n approaches \\(\\infty\\) , the weak law of large numbers indicates that\n\\[\n\\underset{n \\to \\infty}{lim}\\frac{1}{n}\\sum_{i = 1}^{n}(l(Y_i;\\theta)-l_0(Y_i)) = \\mathbb{E}_{f_0}[log(\\frac{f_Y(Y;\\theta)}{f_0(Y)})] = -KL(f_0,f_Y(.;\\theta))\n\\]\nGiven the above\\[\n\\frac{1}{n}\\sum_{i = 1}^{n}(l(Y_i;\\theta)-l(Y_i;\\theta_0))\n\\]\nconverges in probability to \\(KL(f_0,f_Y(.;\\theta_0))-KL(f_0,f_Y(.;\\theta))\\)\nGiven our definition of \\(\\theta_0\\) as the minimizing value of the KL, we can infer that \\(KL(f_0,f_Y(.;\\theta_0))-KL(f_0,f_Y(.;\\theta)) \\leq 0\\)\nhence implying that \\(\\frac{1}{n}\\sum_{i = 1}^{n}(l(Y_i;\\theta)-l(Y_i;\\theta_0))\\) converges to a non-positive constant.\nWe therefore have that\n\\[\nPr_{f_0}[l_n(\\theta_0) \\geq l_n(\\theta)] \\rightarrow 1\n\\]\nAs n approaches \\(\\infty\\) .That is, for increasing n and with probability tending to 1, the log-likelihood of \\(\\theta_0\\) is never less than that of any other \\(\\theta \\in \\Theta\\) .\nIf an identifiability assumption is made, the statement can be strengthened:\nThe model \\(f_Y(y;\\theta)\\) is identifiable if, for two parameter values \\(\\theta^\\dagger = \\theta^\\ddagger\\)\n\\[\nf_Y(y;\\theta^\\dagger) = f_Y(y;\\theta^\\ddagger)\n\\]\nfor all y.\n\\[\nPr_{f_0}[l_n(\\theta_0) &gt;  l_n(\\theta)] \\rightarrow 1\n\\] where \\(\\theta \\neq \\theta_0\\)\nThis holds for fixed \\(\\theta_0\\) in the expression\n\\[\\frac{1}{n}(l_n(\\theta)-l_n(\\theta_0)\\] We also need to study \\(l_n\\hat{\\theta}_n(Y_{1:n}))\\) , where the parameter at which the log-likelihood is evaluated is a random variable: the estimator \\(\\hat{\\theta}n(Y_{1:n})\\).\nWe can show that \\(\\hat{\\theta}n(Y_{1:n}) \\overset{p}{\\rightarrow} \\theta_0\\) and \\(\\hat{\\theta}n(Y_{1:n})\\) is consistent \\(\\theta_0\\) and by “continuous mapping”\n\\[\n|\\frac{1}{n}\\{l_n(\\hat{\\theta}_n(Y_{1:n})) - l_n(\\theta_0)\\} \\overset{p}{\\rightarrow} 0\n\\]\nso that, as \\(n \\to \\infty\\)\n\\[\n\\frac{1}{n}\\sum_{i = 1}^nl(Y_i;\\hat{\\theta}_n(Y_{1:n})) \\overset{p}{\\rightarrow} \\mathbb{E}_{f_0}[l(Y;\\theta_0)]\n\\]\nAsymptotic Normality\nFor a continuous function such as \\(\\dot{l}_n(\\theta)\\) with defined second derivative \\(\\ddot{l}_n(\\theta)\\) , it is guaranteed by the MVT that there exists and intermediate v alue\n\\[\n\\tilde{\\theta} = c\\hat{\\theta}_n + (1-c)\\theta_0$\n\\]\nfor some \\(c, 0 &lt; c &lt; 1\\) such that for\n\\[\n\\dot{l}_n(\\hat{\\theta}_n) = \\dot{l}_n(\\theta_0) + \\ddot{l}_n(\\tilde{\\theta})(\\hat{\\theta}_n - \\theta_0)\n\\]\nThe LHS is zero as $\\hat{\\theta}_n $ is the MLE. Provided \\(\\ddot{l}_n(\\theta)\\) is non-singular, we can write, after rescaling and rearranging that\n\\[\n\\sqrt{n}(\\hat{\\theta}_n - \\theta_0) =  \\{-\\frac{1}{n}\\ddot{l}_n(\\tilde{\\theta})\\}^{-1}\\{\\sqrt{n}\\dot{l}_n(\\theta_0)\\}\n\\]\nIn the RV form, the second term of the RHS can be expressed as\n\\[\n\\sqrt{n}(\\frac{1}{n}\\sum_{i=1}^{n}S(Y_i;\\theta_0)\n\\]\nIn other words, it is a sample average quantity scaled by \\(\\sqrt{n}\\) . However, by definition of \\(\\theta_0\\) we have that\n\\[\n\\mathbb{E}_{f_0}[S(Y;\\theta_0)] = \\int \\dot{l}_n(\\theta_0)f_o(y)dy = O_d\n\\]\nas by definition \\(\\theta_0\\) will minimize the KL and therefore must be a solution to the equation. As a result, by the central limit theorem, we have that\n\\[\n\\sqrt{n}(\\frac{1}{n}\\sum_{i=1}^nS(Y_i;\\theta_0)) \\overset{d}{\\rightarrow} Normal_d(O_d,\\mathcal{I}_{f_0}(\\theta_0))\n\\]\nwhere\n\\[\n\\mathcal{I}_{f_0}(\\theta_0) = \\mathbb{E}_{f_0}[S(Y;\\theta_0)S(Y;\\theta_0)^T] \\equiv Var_{f_0}[S(Y;\\theta_0)]\n\\]\nis a (d x d) quantity.\nAs \\(\\hat{\\theta}_n \\overset{p}{\\rightarrow} \\theta_0\\) we have that\n\\[\n-\\frac{1}{n}\\ddot{l}_n(\\tilde{\\theta}) \\overset{a.s.}{\\rightarrow} \\mathcal{J}_{f_0}(\\theta_0)\n\\]\nTherefore (see slide 195), we have\n\\[\n\\sqrt{n}(\\hat{\\theta}_n - \\theta_0) =  \\{-\\frac{1}{n}\\ddot{l}_n(\\tilde{\\theta})\\}\\{\\frac{1}{\\sqrt{n}}\\dot{l}_n(\\theta_0)\\} + o_p(1)\n\\]\nwhere \\(o_p(1)\\) is a term that converges in probability to zero. The distribution of the second term is given by (slide 196) giving us\n\\[\n\\sqrt{n}(\\hat{\\theta}_n - \\theta_0) \\overset{d}{\\rightarrow} Normal_d(O_d,\\Sigma(\\theta_0))\n\\]\nwhere\n\\[\n\\Sigma(\\theta_0) = \\{\\mathcal{J}_{f_0}(\\theta_0)\\}^{-1}\\mathcal{I}_{f_0}(\\theta_0)\\{\\{\\mathcal{J}_{f_0}(\\theta_0)\\}^{-1}\\}^T\n\\]\nThis follows as, for arbitrary Z, if \\(Var[Z] = V\\) then \\(Var[AZ] = AVA^T\\)\nCorrect specification\nUnder the correct specification,\n\\[\nf_0(y) \\equiv f_Y(y;\\theta_0)\n\\]\nand from earlier results we know that\n\\[\n\\mathcal{I}_{\\theta_0}(\\theta_0) = \\mathcal{J}_{\\theta_0}(\\theta_0)\n\\]\nHence allowing us to deduce that\n\\[\n\\sqrt{n}(\\hat{\\theta}_n - \\theta_0) \\overset{d}{\\rightarrow} Normal_d(O_d,\\{\\mathcal{I}_{\\theta_0}(\\theta_0)\\}^{-1})\n\\]\nUsing the same quadratic approximation for the likelihood at \\(\\theta\\) and \\(\\hat{\\theta}_n\\) we have\n\\[\n\\ell_n(\\theta) \\bumpeq \\ell_n\\left(\\hat{\\theta}_n\\right)+\\dot{\\ell}_n\\left(\\hat{\\theta}_n\\right)^{\\top}\\left(\\hat{\\theta}_n-\\theta\\right)+\\frac{1}{2}\\left(\\hat{\\theta}_n-\\theta\\right)^{\\top} \\ddot{\\ell}_n\\left(\\hat{\\theta}_n\\right)\\left(\\hat{\\theta}_n-\\theta\\right)\n\\]\nBut knowing that \\(\\dot{l}n(\\hat{\\theta}n) = 0\\) we have that\n\\[\n\\begin{aligned}\\exp \\left\\{\\ell_n(\\theta)\\right\\} & \\bumpeq \\exp \\left\\{\\ell_n\\left(\\hat{\\theta}_n\\right)\\right\\} \\exp \\left\\{\\frac{1}{2}\\left(\\hat{\\theta}_n-\\theta\\right)^{\\top} \\ddot{\\ell}_n\\left(\\hat{\\theta}_n\\right)\\left(\\hat{\\theta}_n-\\theta\\right)\\right\\} \\\\& \\propto \\exp \\left\\{-\\frac{1}{2}\\left(\\theta-\\hat{\\theta}_n\\right)^{\\top}\\left\\{-\\ddot{\\ell}_n\\left(\\hat{\\theta}_n\\right)\\right\\}\\left(\\theta-\\hat{\\theta}_n\\right)\\right\\} .\\end{aligned}\n\\]\nThus, when the regularity conditions apply, the likelihood can be approximated by one arising from a Normal distribution\n\\[\nNormal(\\hat{\\theta}_n,\\{-\\ddot{l}_n(\\hat{\\theta}_n)\\}^{-1})\n\\]\nThis approximation can be used in a wide variety of models."
  },
  {
    "objectID": "OptimalDecisionMaking.html#decision-making",
    "href": "OptimalDecisionMaking.html#decision-making",
    "title": "7  Optimal Decision Making",
    "section": "7.1 Decision-Making",
    "text": "7.1 Decision-Making\nMany statistical procedures involve some form of decision-making where actions are taken given the observed data. Examples include\n\nparameter estimation\nhypothesis testing\nprediction/classification\nmodel selection\n\nWe can denote a function \\(T\\) which is used as an estimator of a statistic of interest from the random variable \\(Y\\). Consider, the mean, order statistics, or the empirical cdf. We can also consider the model space \\(\\mathbb{F}\\). We can then examine a loss function \\(L(..)\\) which will determine the accuracy with which we report \\(T\\) when the ground truth comes from \\(F \\in \\mathbb{F}\\).\nFor example, with the cdf \\(F_y\\) we can have that\n\\[\n\\mu = \\int yF_y(dy)\n\\]\nAnd define the loss function as \\(L(T,F_y) = (T - \\mu)^2\\) to represent the loss in reporting the estimator \\(T\\) when the value of interest is \\(\\mu\\).\nThe optimal decision is the decision which minimizes the expected loss with respect to the distribution \\(F_y\\). In a parametric analysis defined by \\(\\theta\\). We have that\n\n\\(\\theta\\) is considered a fixed constant and the data as random in a frequentist setting\nThe data are fixed and \\(\\theta\\) is random in a Bayesian setting\n\n\n7.1.1 Frequentist calculation\nBack to our previous example, in a frequentist setting the optimal decision for \\(T\\) would be\n\\[\nargmin_{\\text{T}}\\mathbb{E}_{F_y}[(T- \\mu)^2] = argmin_{\\text{T}}\\{\\mathbb{E}_{F_y}[(T -\\mathbb{E}_{F_y}[T])^2] + (\\mathbb{E_{F_y}[T] - \\mu)^2}) \\}\n\\]\n\\[\n= argmin_{\\text{T}}\\{Var_{F_y}[T] + (\\mathbb{E_{F_y}[T] - \\mu)^2})\\}\n\\]\nThis tells us that we need to take into account the variance of T and the squared bias to define the optimal T.\n\n\n7.1.2 Kullback-Leibler Divergence/Loss\nIn Bayesian settings, we usually look at the Kullback-Leibler Loss which is used to measure the difference between two distributions \\(F_0\\) and \\(F_1\\).\n\\[\nKL(F_0,F_1) = \\int \\log\\{\\frac{dF_0(y)}{dF_1(y)}\\}dF_0(y)\n\\]\nThis is defined when \\(F_1\\) is absolutely continuous with respect to \\(F_0\\). That is, the probability mass at a given point for \\(F_1\\) is zero whenever it is zero for \\(F_0\\). In essence, we can express the KL as an expectation:\n\\[\nKL(f_0,f_1) = \\mathbb{E}_{f_0}[log\\{\\frac{f_0(Y)}{f_1(Y)}\\}]\n\\]\nIt is important to note that:\n\nKL is always non-negative\n\\(KL(F_0,F_1) \\neq KL(F_1,F_0)\\)\nKL can only be zero if both distributions are identical.\n\nThis is applicable to both discrete and continuous distributions. In a parametric setting, we can expect to have pdf \\(f(y;\\theta)\\) where \\(\\theta_0\\) represents the optimal, data-generating model. We can then write KL as\n\\[\nKL(\\theta_0,\\theta) = \\int\\{\\frac{f(y;\\theta_0)}{f(y;\\theta)}\\}f(y;\\theta_0)dy\n\\]\nand aim to report an estimator \\(\\hat{\\theta} = T(Y)\\) for the true value \\(\\theta_0\\)\n\n\n7.1.3 Decision Theory Concepts\nThe key parts of a decision problem are:\n\na decision d, selected from a set of \\(D\\) decisions must be made.\nthere is a true state of nature, \\(v(\\theta)\\), which lies in the set \\(\\gamma\\) , that is defined bby the data generating model, \\(F_Y(y;\\theta)\\) .\nthere is a loss function \\(L(d,v)\\) for decision \\(d\\) and state \\(v\\) which records the loss when for a given state \\(v\\) and decision \\(d\\).\n\nWith these components, we aim to minimize the expected loss.\nIn the case of estimation, the decision required is the estimation of the parameter \\(\\theta\\) and the true state of nature is the parameter’s value, that is, \\(v(\\theta) \\equiv \\theta\\). If data is available, the optimal decision is usually a function of the observed data. If the decision takes the form of a statistic, we have \\(d(y) \\equiv T(y) = t_n\\) with associated loss \\(L(t_n,\\theta)\\). The corresponding random variable will be \\(T_n \\equiv T(Y)\\).\nFrequentist Risk (loss): The expected loss associated with \\(d(Y)\\) over the distribution of \\(Y|\\theta\\).\n\\[\nR_n(d,\\theta) = \\mathbb{E}_{F_Y}[L(T_n,\\theta)] = \\int_yL(T(y),\\theta)f_Y(y;\\theta)dy\n\\]\nBayes Risk (average): Is the expected risk over the prior distribution:\n\\[\nR_n(d) = \\mathbb{E}_{\\pi_0}[R_n(d,\\theta)] = \\mathbb{E}_{\\pi_0}[\\mathbb{E}_{F_Y}[L(T_n,\\theta)]]\n\\]\n\\[\n= \\int_\\Theta\\{ \\int_yL(t_n,\\theta)f_Y(y;\\theta)dy\\}\\pi_0(\\theta)d\\theta  \n\\]\nwhere \\(t_n \\equiv t_n(y)\\).\n\\[\n= \\int_\\Theta\\int_yL(t_n,\\theta)f_Y(y)\\pi_n(\\theta)dyd\\theta\n\\]\n\\[\n= \\int_y\\{\\int_\\Theta L(t_n,\\theta)\\pi_n(\\theta)d\\theta\\}f_Y(y)dy\n\\]\nFollowing Bayes Theorem.\nWith the prior and fixed observable data, the optimal Bayesian decision, the Bayes rule is\n\\[\n\\hat{d}_B = argmin_{d \\in D} R_n(d(y))\n\\]\nInterpretation:\n\nBayes risk is minimized when the inner integral is minimized for fixed observations, regardless of their value\nThe double integral is minimized\nor that the optimal decision should be chosen conditionally on the observed data and not averaged over all possible values.\n\nTo estimate, we need to select a statistic of interest. The minimization can be reduced to the following:\n\\[\nargmin_{t \\in \\Theta}\\int_\\Theta L(t_n,\\theta)\\pi_n(\\theta)d\\theta\n\\]\nGiven the interpretation mentioned above. In other words, the optimal decision minimizes the Bayes risk and the posterior expected loss.\n\n7.1.3.1 Squared-error loss\n\nReturns the posterior expectation (see proof)\n\n\n\n7.1.3.2 Absolute error loss\n\nReturns the posterior median\n\n\n\n7.1.3.3 Zero-one loss\n\nReturns the posterior mode"
  },
  {
    "objectID": "PosteriorApproximation.html",
    "href": "PosteriorApproximation.html",
    "title": "8  Approximation and Sampling Methods",
    "section": "",
    "text": "Previously we have looked at Bayesian models with closed form solutions for the marginal likelihood and posterior. However, in cases where more complex models are required, the marginal likelihood is often not closed form making it impossible to solve the posterior analytically.\nIn this cases, we are required to approximate the posterior from which we can then compute the statistics of interest which we saw in the previous section.\nThere are two key ways to approximate the posterior:\n\nSampling/Simulation: we can generate random samples from the posterior distribution to then use the empirical results as an estimate of the posterior.\nVariational Inference: we can approximate the posterior by using a closed-form surrogate distribution.\n\nA simple form of variational inference is normal approximation, where the central limit theorem is used to justify the use of the normal distribution as a surrogate posterior. In this chapter we will focus on sampling methods for posterior approximation.\n\n8.0.1 Sampling Methods\nTo approximate the posterior by simulation, we need to generate random samples from the posterior. This is trivial if the posterior is a known distribution with a derived simulation method. However, this becomes interesting when the posterior isn’t closed form. We can achieve this by making use of the unnormalized posterior density or any function \\(q(\\theta;y)\\) that is proportional to it.:\n\\[\np(\\theta|y) \\propto p(\\theta)p(y|\\theta) \\propto q(\\theta;y)\n\\]\nand generating random samples from this to approximate the posterior.\nTo generate the random samples, we can use rejection sampling or importance sampling for simple models. There are also many probabilistic programming tools available to automatically generate simulations.\n\n\n8.0.2 Grid Approximation\nThis method, also know as direct discrete approximation, works as follows:\n\nCreate an even-spaced grid \\(g_1 = a + i/2,…,g_m = b - i/2\\) where \\(a\\) and \\(b\\) are the lower and upper bounds of the parameter space of the posterior and \\(i\\) and \\(m\\) are the grid increments and number of grid points respectively.\nEvaluate the values of the unnormalized posterior density across the grid points \\(q(g_j;y)\\) and normalize them by their sum to estimate the posterior values:\n\\[\n\\hat{p_j} = \\frac{q(g_j;y)}{\\sum_{i = 1}^{m}q(g_i;y)}\\]\nFor every iteration \\(s = 1,…,S:\\)\n\nGenerate a sample for \\(\\theta_s\\) from a categorical distribution with \\(m\\) outcomes ($g_1,…,g_m$) with respective probabilities \\(\\hat{p}_1,...,\\hat{p}_m\\)\nAdd zero-centered uniformally distributed noise \\(\\epsilon\\) that has an interval length equal to the grid interval spacing such that:\n\\[\\hat{\\theta}_s = \\theta_s + \\epsilon\\]\nwhere \\(\\epsilon \\sim Uniform(-i/2,i/2)\\)\n\n\nThis generates a process similar to numerical integration, meaning that this approximation approach is limited to a finite interval.\nNote: I have been strictly using \\(\\theta\\) as the symbol for parameters across the different models used to make it clear what symbol represents the distribution parameter. In practice, different distributions make use of different symbols for their parameter. Examples are \\(\\alpha, \\beta, \\lambda\\) some of which were used in previous sections when defining the prior distributions.\n\n8.0.2.1 Example\nWe can demonstrate the usage of grid approximation with the Poisson-gamma model we explored previously. Simulation isn’t necessary here since the posterior is closed-form, but this will at least demonstrate how grid approximation is able to approximate it.\nRecall that the posterior of the Poisson-Gamma model had the following form:\n\\[\n\\Theta|Y \\sim Gamma(\\alpha + s_n,\\beta + n)\n\\]\n\ntrue_theta &lt;- 3\na &lt;- b &lt;- 1\nn &lt;- 5\nset.seed(123)\n\ny &lt;- rpois(n,true_theta)\n#Un-normalized Posterior (numerator for gamma distribution)\nq &lt;- function(theta,y,n,alpha,beta){\n  theta^(alpha + sum(y) - 1) * exp(-(n + beta)*theta)\n}\n\n#Explore theta on (0,25) --&gt; Need a fixed range \nlow_bound &lt;- 0\nhigh_bound &lt;- 20\n#Grid increment\ni &lt;- 0.01\ngrid &lt;- seq(low_bound + i/2, high_bound - i/2,by = i)\nn_sim &lt;- 10000\ngrid_n &lt;- length(grid)\ngrid_val &lt;- q(grid,y,n,a,b)\n#Normalize to create a proper categorical distribution\ngrid_norm &lt;- grid_val/sum(grid_val)\n\n## Simulate values\nsim_idx &lt;- sample(1:grid_n,n_sim,prob = grid_norm,replace = TRUE)\n\n#Noise parameter\ne &lt;- runif(n_sim,-i/2,i/2)\ntheta_sim &lt;- grid[sim_idx] + e\n\npar(mfrow = c(1,1),mar = c(4,4,1.5,1.5))\n## Visualize with histogram\nhist(theta_sim,col = 'orange',breaks = seq(0,8,0.25), probability = TRUE,\n     main = 'Grid Approximation using Poisson-Gamma Model',\n     xlab = expression(theta),xlim = c(0,6))\nlines(grid,dgamma(grid,a + sum(y),b + n),col = 'lightblue',lwd = 4)\nlegend('topright', legend = 'Ground Truth', col = 'lightblue',lwd = 4)\n\n\n\n## Visualize with density estimate\nplot(grid,dgamma(grid,a + sum(y),b + n), type = 'l',col = 'lightblue',lwd = 4, ylab = 'Density',\n     xlab = expression(theta),xlim = c(0,6))\nlines(density(theta_sim),col = 'orange',lwd = 4)\nlegend('topright', legend = c('Estimate','Ground Truth'), col = c('lightblue','orange'),lwd = 4)\n\n\n\n\nNote that we needed to explore \\(\\theta\\) on a fixed interval (0,25) to be able to approximate it. We can determine the plausible range of values to explore using the prior (if it is informative) or the likelihood. The key requirement is that we make sure the interval covers nearly all of the pmf for the distribution.\nWe can clearly see that the grid approximation has allowed us to correctly approximate the posterior without having to evaluate it analytically.\n\n\n8.0.2.2 Example #2 : Non-conjugate priors\nLet’s look at an example of a non-conjugate distribution, where the normalizing constant is such that the posterior is not a known distribution and is therefore not closed-form (intractable). A common example is using a log-normal prior for a Poisson likelihood. In essence, if our random variable is \\(X\\) and is normally distributed, we can use a reparameterization of the r.v, \\(Y = e^X\\) to get a log-normal distribution with the mean and variance being equal to the log of the mean and variance for \\(X\\) .\nOur model therefore uses the following distributions:\n\\[\nY \\sim Poisson(\\theta)\n\\]\n\\[\n\\theta \\sim Log  normal(\\mu,\\sigma^2)\n\\]\nThe pdf for the prior is as follows:\n\\[\np(\\theta) = \\frac{1}{\\theta \\sqrt{2\\pi\\sigma^2}}e^{-\\frac{(log\\theta - \\mu)^2}{2\\sigma^2}},\n\\]\nwhich gives the following un normalized posterior pdf\n\\[\np(\\theta|y) \\propto p(\\theta)p(y|\\theta)\n\\]\n\\[\n\\propto \\theta^{-1}\ne^{-\\frac{(log\\theta - \\mu)^2}{2\\sigma^2}}\\theta^{\\sum_{i = 1}^{n}y_i}e^{-n\\theta}\n\\]\n\\[\n\\propto\n\\theta^{\n{\\sum_{i = 1}^{n}y_i}-1}e^{-n\\theta\n- {\\frac{(log\\theta - \\mu)^2}{2\\sigma^2}}\n}\\]\nAnd a marginalized joint pdf (numerator) we cannot integrate over (intractable). We can now apply grid approximation! We will be reusing the distribution we create in the last example.\n\n#Approximating the distribution we generated in the previous example\nq &lt;- function(theta,y,n,mu,sigma2){\n  theta^(sum(y) - 1) * exp(-n*theta-(log(theta)-mu)^2 / (2*sigma2))\n}\nmu &lt;- 0\nsigma2 &lt;-1\n#Explore theta on (0,25) --&gt; Need a fixed range \nlow_bound &lt;- 0\nhigh_bound &lt;- 20\n#Grid increment\ni &lt;- 0.01\ngrid &lt;- seq(low_bound + i/2, high_bound - i/2,by = i)\ngrid_val &lt;- q(grid,y,n,mu,sigma2)\ngrid_norm &lt;- grid_val/sum(grid_val)\n\n## Simulate values\nsim_idx &lt;- sample(1:grid_n,n_sim,prob = grid_norm,replace = TRUE)\n#Noise parameter\ne &lt;- runif(n_sim,-i/2,i/2)\ntheta_sim &lt;- grid[sim_idx] + e\n\n## Visualize\npar(mfrow = c(1,1),mar = c(4,4,1.5,1.5))\n#plot 1\nhist(theta_sim,col = 'orange',breaks = seq(0,8,0.25), probability = TRUE,\n     main = 'Grid Approximation using Poisson-LogNormal',\n     xlab = expression(theta),xlim = c(0,8),ylim = c(0,0.75))\nlines(grid,dgamma(grid,a + sum(y),b + n),col = 'lightblue',lwd = 4)\nlegend('topright', legend = 'Ground Truth', col = 'lightblue',lwd = 4)\n\n\n\n##plot 2\n\nplot(grid,dgamma(grid,a + sum(y),b + n), type = 'l',col = 'lightblue',lwd = 4, ylab = 'Density',\n     xlab = expression(theta),xlim = c(0,8),ylim = c(0,0.75))\nlines(density(theta_sim),col = 'orange',lwd = 4)\nlegend('topright', legend = c('Estimate','Ground Truth'), col = c('lightblue','orange'),lwd = 4)\n\n\n\n\nWe can see that our approximation is close to the ground truth, but is slightly off. This could be seen as an indication that the selected prior may not be the best option (see result with gamma prior above)!\n\n\n\n8.0.3 Monte Carlo Methods\nWe have now seen that a sufficient number of simulations can approximate the ground truth posterior. These simulations can also be used to calculate summary statistics such as the posterior mean, variance and credible intervals!\nIn general, computing integrals via simulations is referred to as Monte Carlo integration or the Monte Carlo method. These types of methods depend on the strong law of large numbers (SLL).\n\n8.0.3.1 Strong law of large numbers\nWhere we have a sequence of random variables \\(Y_i,…,Y_n\\) that are i.i.d. with a finite expected value \\(\\mu\\) , we have that almost surely (a.s.):\n\\[\n\\lim_{n\\to\\infty} \\frac{1}{n}\\sum_{i = 1}^{n}y_i = \\mu\n\\]\nWe are able to say almost surely as the above statement is said to converges to the result with a probability of one.\nSLL implies that the sample mean of an i.i.d sequence of random variables will always converge to the expected value of the underlying distribution with sufficient sample size. In the case of coin tosses, we would therefore expect that after enough trials, by SLL, that the expected value is 0.5 (assuming it is an unbiased coin!).\n\n\n8.0.3.2 Example: Monte Carlo Integration\nWe can revisit our example from the beginning of the chapter where we estimate \\(\\theta\\). By SLL, we expect that that average of the simulated values will converge to the true posterior mean of \\(\\theta\\) provided we simulate enough values. We can therefore approximate the posterior expectation using this mean. In this case, we know the posterior expectation of the Poisson-gamma model\n\\[\n\\mathbb{E}[\\Theta|Y = y] = \\frac{\\alpha_n}{\\beta_n} = \\frac{\\sum_{i = 1}^{n}Y_i +\\alpha}{n + \\beta}\n\\]\nso we can validate our approximation.\n\na_n &lt;- a + sum(y)\nb_n &lt;- b + n\n#True Expected Posterior\na_n/b_n\n\n[1] 3.333333\n\n#Simulated Posterior Mean\nmean(theta_sim)\n\n[1] 3.548937\n\ntheta_est &lt;- cumsum(theta_sim)/1:length(theta_sim)\nplot(1:length(theta_sim),theta_est,ylim = c(2.5,3.6),\n     ylab = expression(theta),\n     xlab = 'N')\nabline(h = a_n/b_n,col = 'red')\n\n\n\n\nThe same can be done to approximate the true posterior variance.\n\n#True Posterior variance\na_n/(b_n^2)\n\n[1] 0.5555556\n\n#Simulated Posterior variance\nvar(theta_sim)\n\n[1] 0.688096\n\n\nWe can also approximate posterior probabilities within an interval defined by an indicator \\(I_{(a,b)}(x)\\) where \\(I_{(a,b)}(x) = 1\\) if \\(x \\in (a.b)\\) and \\(I_{(a,b)}(x) = 0\\) otherwise:\n\n#True Posterior\npgamma(3,a_n,b_n,lower.tail = FALSE)\n\n[1] 0.6509161\n\n#Simulated Posterior\nmean(theta_sim &gt; 3)\n\n[1] 0.7323\n\n\nTherefore, we are also able to estimate quantiles:\n\nconf &lt;- 0.05\n\n#True 95% credible interval lower bound\nqgamma(conf/2,a_n,b_n)\n\n[1] 2.036087\n\n#Estimated 95% credible interval lower bound\nquantile(theta_sim,conf/2)\n\n    2.5% \n2.127166 \n\n\n\n#True 95% credible interval upper bound\nqgamma(1- conf/2,a_n,b_n)\n\n[1] 4.945142\n\n#Estimated 95% credible interval upper bound\nquantile(theta_sim,1- conf/2)\n\n   97.5% \n5.346038 \n\n\n\n\n\n8.0.4 Importance Sampling\nIn cases where we can’t generate samples from the target distribution, Monte Carlo methods allow us to generate viable samples by means of a surrogate distribution. In essence, we aim to identify a pdf \\(f_0\\) for which sampling is possible to then estimate the features of interest of our target distribution’s pdf \\(f\\). For this to work, it is imperative that the selected pdf has the support for the same domain as the target pdf. In the case of determining an expected value we get:\n\\[\n\\int{g(x)f(x)}dx = \\int{g(x)f(x)\\frac{f_0(x)}{f_0(x)}}dx =  \\int{\\frac{g(x)f(x)}{f_0(x)}f_0(x)}dx\n\\]\nWhich gives us\n\\[\n\\mathbb{E}_f[g(X)] = \\mathbb{E}_{f_0}[\\frac{g(X)f(X)}{f_0(X)}]\n\\]\nUsing the SLL as discussed previously, we can then use the estimator \\(\\hat{I}^{(f_0)}_N(g)\\) to get the expected value over the target pdf:\n\\[\\hat{I}^{(f_0)}_N(g) = \\frac{1}{N}\\sum_{i=1}^{N}\\frac{g(X_i)f(X_i)}{f_0(X_i)}\\]\nIn practice, \\(\\hat{I}^{(f_0)}_N(g)\\) is known as the importance sampling estimator. To emphasize the notion of importance, we can rearrange the equation as follows:\n\\[\\hat{I}^{(f_0)}_N(g) = \\frac{1}{N}\\sum_{i=1}^{N}\\frac{f(X_i)}{f_0(X_i)}g(X_i) = \\sum_{i=1}^{N}w_0(X_i)g(X_i)  \\]\nWhere \\(w_0(X_i)\\) represents the importance sampling weight. We would choose to use the importance sampler estimator over the standard estimator in cases where its variance is less than that of the standard estimator.\nLet’s look at an example. Let us say that we want to get the following expectation over \\(f(x)\\)\n\\[\nE_f[g(x)]\n\\]\nWhere we know that \\(f(x)\\) is the pdf of a Gamma distribution \\(X \\sim Gamma(2,3)\\). We aim to compute \\(\\mathbb{E}[X^3]\\). As we know the ground truth, we know that the expected value for the random variable of a gamma function is:\n\\[\\mathbb{E}[X^r] = \\frac{1}{\\beta^r}\\frac{\\Gamma(\\alpha + r)}{\\Gamma(\\alpha)}\\]\n\na &lt;- 2\nb &lt;- 3\nr &lt;- 4\ntrue_expec &lt;- gamma(a + r)/((b^r) *gamma(a))\nx &lt;- seq(0,5,by = 0.01)\ny &lt;- x^r\npar(mar=c(3,3,2,2))\nplot(x,y*dgamma(x,2,3),type = 'l',col = 'red',\n     xlab = 'X',ylab = '',ylim = c(0,1.2))\nlines(x,dgamma(x,2,3),col = 'blue')\nlegend('topright',legend = c('g(x)f(x)','f(x)'),col = c('red','blue'))\n\n\n\n\nWe can now evaluate importance sampling estimators that use a series of \\(f_0\\) functions:\n\n\\(Gamma(2,3)\\) - Standard MC Estimator\n\\(Gamma(6,3)\\)\n\\(Student(5)\\) centered around \\(x = 2\\)\n\\(Normal(3,2)\\)\n\\(Gamma(2,6)\\)\n\nDoing this for multiple runs will help us determine the variance of each estimator.\n\nruns &lt;- 1000\nsamples &lt;- 10000\nestimates &lt;- matrix(0,nrow = runs,ncol = 5)\nfor (run in 1:runs){\n  X1 &lt;- rgamma(samples,a,b)\n  X2 &lt;- rgamma(samples,6,3)\n  X3 &lt;- rt(samples,5) + 2\n  X4 &lt;- rnorm(samples,3,2)\n  X5 &lt;- rgamma(samples,5,2)\n  Y1 &lt;- (X1^r)\n  Y2 &lt;- (X2^r)*dgamma(X2,a,b)/dgamma(X2,6,3)\n  Y3 &lt;- (X3^r)*dgamma(X3,a,b)/dt(X3-2,5)\n  Y4 &lt;- (X4^r)*dgamma(X4,a,b)/dnorm(X4,3,2)\n  Y5 &lt;- (X5^r)*dgamma(X5,a,b)/dgamma(X5,5,2)\n  estimates[run,] &lt;- c(mean(Y1),mean(Y2),\n                       mean(Y3),mean(Y4),mean(Y5))\n}\npar(mar=c(3,3,2,2))\nboxplot(estimates)\ntitle('Comparison of IS Estimators over 1000 Replicates')\nabline(h = true_expec,col = 'red')\n\n\n\n\nAs we can see, the standard MC estimator actually has a lot more uncertainty than the IS approaches. We notice that the estimators using a gamma pdf work quite well.\n\n\n8.0.5 Optimal Importance Sampling\nWhen choosing \\(f_0(x)\\), we want to ensure that the variance is finite so that our confidence on the estimate is quantifiable. The estimator’s variance is finite if and only if\n\\[\n\\frac{g(x)f(x)}{f_0(x)}\n\\]\nhas finite variance. In other words, it is finite if\n\\[\n\\mathbb{E}_{f_0}[\\{\\frac{g(x)f(x)}{f_0(x)}\\}^2] = \\int_{-\\infty}^\\infty\\{\\frac{g(x)f(x)}{f_0(x)}\\}^2f_0(x)dx\n\\]\nis finite. We therefore conclude that an optimal choice for \\(f_0\\) occurs when\\(f_0(x) \\propto |g(x)|f(x)\\)\nWhen the ratio \\(\\frac{f(x)}{f_0(x)}\\) has unbounded support, the variance of our estimator will not be finite. We therefore need a way to ensure that the ratio remains bounded at the tails for \\(f\\) with unbounded support. In practice, we find that\n\\[\n\\lim_{n\\to\\infty}\\frac{1}{N}\\sum_{i = 1}^N\\frac{f(X_i)}{f_0(X_i)} = \\int_{-\\infty}^\\infty\\frac{f(X_i)}{f_0(x)}f_0(x)dx = 1\n\\]\nallow us to derive the following estimator\n\\[\n\\tilde{I}_N^{(f_0)}(g) = \\frac{\\sum_{i=1}^{N}\\frac{g(X_i)f(X_i)}{f_0(X_i)}}{\\sum_{i = 1}^N\\frac{f(X_i)}{f_0(X_i)}}\\xrightarrow{\\text{a.s.}} \\mathbb{E}_f[g(X)]\\]\nWhich, if it exists, may have a smaller variance than the variance we saw earlier.\n-Note: consider adding something about harmonic mean derivation\n\n\n8.0.6 Rejection Sampling\n\ntrue_dist &lt;- function(x){   return(dnorm(x,30,10)+ dnorm(x,80,20))  } \nestim_dist &lt;- function(x){   return(dnorm(x,50,30)) } \nx &lt;- seq(-50,150) \nc &lt;- max(true_dist(x)/estim_dist(x)) \nplot(x,true_dist(x),type = 'l',ylim = c(0,0.06)) \nlines(x,c*estim_dist(x)) \n\n\n\n\n\nrej_sample &lt;- function(n,c){   \n  x &lt;- rnorm(n,50,30)  \n  k &lt;- runif(n)   \n  return(x[true_dist(x)/(c*estim_dist(x)) &gt; k]) }\nrej_x &lt;- rej_sample(100000,c) \nhist(rej_x,freq = F, ylim = c(0,0.02)) \nlines(x = density(x = rej_x))\n\n\n\n\n\n\n8.0.7 Sampling Importance Resampling\nDespite being similar, there is a key difference between importance and rejection sampling:\n\nImportance sampling focuses on approximating numerical integration with respect to \\(f(x)\\)\nRejection sampling focuses on generating i.i.d samples from \\(f(x)\\)\n\nBased on these differences, we can consider Sampling Importance Resampling (SIR) which can sample from density \\(f(x)\\) by re-weighting and then resampling samples from \\(f_0(x)\\):\n\nGenerate samples \\(x_1,…,x_n\\) from \\(f_0\\)\nCalculate re-normalized weights \\(w_1,…,w_n\\) \\[w_i = \\frac{\\frac{f(X_i)}{f_0(X_i)}}{\\sum_{j = 1}^N\\frac{f(X_j)}{f_0(X_j)}} \\]\nGenerate new samples (resample) from the discrete distribution of the samples \\(x\\) using the weights \\(w\\) as the probability masses.\n\nNote: make an example\n\n\n8.0.8 Markov Chain Monte Carlo (MCMC) Methods\nGrid approximation worked well in our 1-dimensional example. However, this is not always the case when we begin to work with high-dimensional data. To cover the parameter space we defined in our example, we generated a grid with 2000 points. However, if we had additional dimensions each with 2000 points, we would find ourselves working with \\(2000^d\\) grid points. This becomes quite impractical in what has been a very simple example. Clearly, grid approximation will not be viable for higher dimensions.\nAs a result, most sampling approaches for more complex models usually make use of Markov Chain Monte Carlo (MCMC) methods. This family of methods focuses on iterative sampling from a markov chain for which the distribution at convergence matches the target distribution. In our case, this is the posterior distribution.\n\n8.0.8.1 Markov Chain\nA Markov chain is a sequence of random variables \\(X_1,X_2,…\\) that have a Markov property. That is, for any discrete time point \\(i\\) :\n\\[P(X_{i+1} | X_i,X_{i-1},...,X_0) = P(X_{i+1} | X_i)\\]\nThat is to say, for any discrete time point $i$, the state of the random variable at the next time point, \\(X_{i+1}\\) depends only on the current state \\(X_i\\) . The set of all possible values or states that the random variables can take is known as the state space \\(S\\).\n\n\n8.0.8.2 MCMC Sampling\nSimplistic sampling methods such as the ones we saw previously generate i.i.d. samples from the target distribution. However, the values we sample from the state space S (\\(\\theta_1,…,\\theta_S\\)) by an MCMC method experience high auto-correlation. That is, the value sampled at the next time point will be similar to the current value. Although this would be an issue for a small sample size, MCMC overcomes this by generating a large number of samples and using the full sample to approximate the posterior distribution. MCMC algorithms converge when they reach the stationary distribution. That is, the stationary distribution is a distribution \\(\\pi(x)\\) achieved at a state where for all \\(i \\in \\mathbb{Z}^+\\) :\n\\[\nP(X_i = k) = \\pi(k)\n\\]\nwhich implies that the next state of the random variable is the same as the current state. Once it has reached convergence, sampling from the stationary distribution will eventually lead to an approximation of the true posterior. Determining when convergence has been reached is difficult to determine. There are different diagnostics available to explore this, one of which is to running different MCMC instances with different initializations and see if they reach a similar distribution.\nAs the first iterations of MCMC sampling do not represent the posterior yet, they are usually discarded. The number of iterations that are discarded is up to the user and is referred to as the burn-in period. Markov chains that were designed to estimate target posterior distributions are known as MCMC samplers. The two most popular examples are the Gibbs sampler and the Metropolis-Hastings sampler. The Gibbs sampler is actually a special case of the MH sampler.\n\n\n8.0.8.3 Example: Gibbs sampler\nThe Gibbs sampler works by incrementally updating each of the components of the parameter vector \\(\\theta\\). We can assume that the parameter vector is multi-dimensional where for each component \\(\\theta_j\\) , the sampler generates a value using the conditional posterior distribution of the component given all the other components.\n\\[\np(\\theta_j | \\theta_{-j},y)\n\\]\nNotice that the current component is omitted from the parameter vector being used as prior information!\nWe can explore the concept with a two-dimensional example. We assume that we have one observation \\((y_1,y_2) = (0,0)\\) which comes from a two-dimensional normal distribution \\(N(\\mu,\\Sigma_0)\\) , where the parameter we are interested in is the mean vector \\(\\mu = (\\mu_1,\\mu_2)\\) and the covariance matrix\n\\[\\Sigma_0 =\n\\begin{bmatrix}\n1 & \\rho \\\\\n\\rho & 1 \\\\\n\\end{bmatrix} \\] is assumed to be known and constant with \\(\\rho = -0.7\\) . We can also assume that we are using an improper uniform prior \\(p(\\mu) \\propto 1\\). The posterior is now a two-dimensional normal distribution \\(N(\\mu,\\Sigma_0)\\). Note that for now we will not consider the inference of the posterior. Normally we could generate samples from pre-existing implementations but we will look at making a rudimentary implementation of a Gibbs sampler.\nWe have the following conditional posteriors for each mean of interest:\n\\[\n\\mu_1|\\mu_2,Y \\sim N(y_1 + \\rho(\\mu_2-y_2),1 - \\rho^2)\n\\]\n\\[\n\\mu_2|\\mu_1,Y \\sim N(y_2 + \\rho(\\mu_1-y_1),1 - \\rho^2)\n\\]\n\ny &lt;- c(0,0)\nrho &lt;- -0.7\nset.seed(111)\nmu1_sample &lt;- function(y, rho, mu2) rnorm(1, y[1] + rho * (mu2-y[2]), sqrt(1-rho^2))\nmu2_sample &lt;- function(y, rho, mu1) rnorm(1, y[2] + rho * (mu1-y[1]), sqrt(1-rho^2))\n\nn_sim &lt;- 100\nmu1 &lt;- mu2 &lt;- numeric(n_sim)\nmu1[1] &lt;- mu2[1] &lt;- 2\nfor (i in 2:n_sim){\n  mu1[i] &lt;- mu1_sample(y,rho,mu2[i-1])\n  mu2[i] &lt;- mu2_sample(y,rho,mu1[i])\n}\n\nlibrary(ggplot2)\n\nWarning: package 'ggplot2' was built under R version 4.2.3\n\nlibrary(gganimate)\n\nWarning: package 'gganimate' was built under R version 4.2.3\n\ngibbs_df &lt;- data.frame(mu1,mu2, iteration = 1:n_sim)\ngibbs_df$start &lt;- '-'\ngibbs_df$start[1] &lt;- \"Start\"\n\nggplot(gibbs_df,aes(mu1,mu2,shape = start)) + \n  geom_point() + geom_path(aes(group = 'all'))  + \n  theme(panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank(),\npanel.background = element_blank(),\naxis.line = element_line(colour = \"black\"))\n\n\n\nggplot(gibbs_df,aes(mu1,mu2,shape = start)) + \n  geom_path( aes(group = 'all'))  + \n  theme(panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank(),\npanel.background = element_blank(),\naxis.line = element_line(colour = \"black\")) +\n  labs(title = 'Iteration: {frame_along}') +\n  transition_reveal(along = iteration)\n\n`geom_path()`: Each group consists of only one observation.\nℹ Do you need to adjust the group aesthetic?\n`geom_path()`: Each group consists of only one observation.\nℹ Do you need to adjust the group aesthetic?\n\n\n\n\nanim_save('mcmc.gif', animation = last_animation(), path = NULL)\n\nWe can compare our results with the true posterior to see how well we approximated the distribution.\n\ncovar &lt;- matrix(c(1,rho,rho,1),ncol =2)\n\nX &lt;- MASS::mvrnorm(n_sim,y,covar)\n\npar(mfrow = c(1,2))\n plot(mu1, mu2, pch = 16, col = 'orange',\n       xlim = c(-4,4), ylim = c(-4,4), asp = 1, xlab = expression(mu[1]), \n       ylab = expression(mu[2]),\n      main = 'MCMC')\n  plot(X[,1], X[,2], pch = 16, col = 'orange',\n       xlim = c(-4,4), ylim = c(-4,4), asp = 1, xlab = expression(mu[1]), \n       ylab = expression(mu[2]),\n       main = 'True Posterior')\n\n\n\n\nAs we can see, despite starting with estimates well off of the true values, the Gibbs sampler was able to reach a distribution that is quite similar to the true posterior.\n\n\n\n8.0.9 Metropolis-Hastings\nAn alternative method for MCMC sampling is the Metropolis-Hastings algorithm. In this approach, we iteratively propose a random sample of the random variable as the next state \\(z\\) given our current state \\(x\\). This is reffered to as a transition kernel \\(Q(z|x)\\) which is often simply denoted as a normal distribution with the current state \\(x\\) as the mean and a standard deviation equal to 1. By defining criteria that allows us to accept or reject the proposed new sample, we are able to explore the domain of possible values generated by the target distribution through what is a random walk (think Brownian motion). The criteria for acceptance is known as the acceptance probability and is calculated as follows:\n\\[\nA = min(1,\\frac{\\pi(z)Q(x|z)}{\\pi(x)Q(z|x)}\n\\]\nIn the case of a transition kernel which has a random walk behavior as the one described above, we find that \\(Q(z|x) = Q(x|z)\\) allowing us to exclude both from the calculation of the acceptance probability.\nTo achieve this in our multi-dimensional setting, we would make use of the following steps:\n\nInitialize the state \\(X_1 = (\\mu_1,\\mu_2)\\)\nFor t steps:\n\nsample \\(z\\) from the transition kernel \\(Q(z|x)\\)\nCompute the acceptance probability\nAccept or reject the proposed state\n\n\nWe can apply this to our previous distribution ( \\(\\mu_1= \\mu_2 =0\\) ) to see how well we can replicate its sampling.\n\nt &lt;- 10000\nx &lt;- matrix(0,nrow = 2,ncol = t)\nx[,1] &lt;- c(-2,2)\nz &lt;- c(0,0)\nrho &lt;- -0.7\ncovar &lt;- matrix(c(1,rho,rho,1),ncol =2)\nfor (i in 2:t){\n  z[1] &lt;- rnorm(1,x[1,i-1])\n  z[2] &lt;- rnorm(1,x[2,i-1])\n  A &lt;- min(1,mvtnorm::dmvnorm(z,mean = rep(0,2),sigma = covar))\n  if (runif(1) &lt; A){\n    x[,i] &lt;- z\n  } else {\n    x[,i] &lt;- x[,i-1]\n  }\n}\nx &lt;- as.data.frame(t(x))\nggplot(x,aes(V1,V2)) + geom_point()  + \n  theme(panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank(),\npanel.background = element_blank(),\naxis.line = element_line(colour = \"black\")) +\n  labs(title = 'Metropolis-Hastings Algorithm Samples',\n       x = expression(mu[1]),\n       y = expression(mu[2]))\n\n\n\n\nOnce again, we see that the MCMC sampler is generating samples in a similar fashion to the target distribution!\n\n\n8.0.10 Probabilistic Programming - Incomplete"
  },
  {
    "objectID": "BayesianModels.html",
    "href": "BayesianModels.html",
    "title": "9  Bayesian Modelling",
    "section": "",
    "text": "10 Regression Models\nWe can consider an infinite sequence \\(\\{ (X_n,Y_n), n = 1,2,…\\}\\) such that for any \\(n \\geq 1\\)\n\\[\nf_{X_1,...,X_n,Y_1,...,Y_n}(x_1,...,x_n,y_1,...,y_n)\n\\]\ncan be factorized as\n\\[\nf_{X_1,...,X_n}(x_1,...,x_n)f_{Y_1,...,Y_n|X_1,...,X_n}(y_1,...,y_n|x_1,...,x_n)\n\\]\nwhere each term has a deFinetti representation.\n\\[\nf_{X_1,...,X_n}(x_1,...,x_n) = \\int \\{\\prod_{i=1}^nf_X(x_i;\\phi)\\}\\pi_0(d\\phi) \\\\\nf_{Y_1,...,Y_n|X_1,...,X_n}(y_1,...,y_n|x_1,...,x_n) = \\int \\{\\prod_{i=1}^nf_{Y|X}(y_i|x_i;\\theta)\\}\\pi_0(d\\theta)\n\\]\nGiven the above structure, inference for \\((\\phi,\\theta)\\) is required:\nFor the latter, the fact that X is random is irrelevant as we have conditioned the model on observed values of X.\nWhen considering the statistical behaviour of Bayesian (and frequentist) procedures, we need to remember that X and Y have a joint structure.\nPrediction\n\\[\nf_{Y_{n+1}|X_{1:n},Y_{1:n}}(y_{n+1}|x_{1:n},y_{1:n}) \\\\\n= \\int f_{X_{n+1},Y_{n+1}|X_{1:n},Y_{1:n}}(x_{n+1},y_{n+1}|x_{1:n},y_{1:n})dx_{n+1} \\\\\n= \\int f_{Y_{n+1}|X_{1:n},X_{n+1},Y_{1:n}}(y_{n+1}|x_{1:n},x_{n+1},y_{1:n})f_{X_{n+1}|X_{1:n},Y_{1:n}}(x_{n+1}|x_{1:n},y_{1:n})dx_{n+1}\n\\]"
  },
  {
    "objectID": "BayesianModels.html#linear-regression",
    "href": "BayesianModels.html#linear-regression",
    "title": "9  Bayesian Modelling",
    "section": "9.2 Linear Regression",
    "text": "9.2 Linear Regression\nWe can start with the following linear regression model\n\\[\nY_i = x_i\\beta + \\epsilon_i\n\\]\nwhere for \\(i = 1,…,n\\)\n\n\\(Y_i\\) is a scalar\n\\(x_i\\) is (1 x d)\n\\(\\beta\\) is (d x 1)\n\\(\\epsilon_i \\sim Normal(0,\\sigma^2)\\), independently.\n\nWith this structure, we can describe the model for the partially exchangeable random variables (error terms) \\(\\epsilon_i = Y_i - x_i\\beta\\), conditional on \\(X_i = x_i\\). In this scenario, there may or may not be a need to model the distribution of \\(X_i\\). –&gt; Note: figure out why.\nWe can look at the vector form of the linear regression model\n\\[\nY = X\\beta + \\epsilon\n\\]\nwhere the response variable and the error terms are (n x 1) vectors and the predictors are an (nxd) matrix.\nWe can then have a conditional model\n\\[\nf_{Y_1,...,Y_n|X_1,...,X_n}(y_1,...,y_n|x_1,...,x_n;\\beta,\\sigma^2) \\equiv Normal_n(X\\beta,\\sigma^2I_n)\n\\]\nwhere \\(I_n\\) is an identity matrix (nxn).\nWith this structure, we know the likelihood to be\n\\[\n\\mathcal{L}_n(\\beta,\\sigma^2) = (\\frac{1}{2\\pi\\sigma^2})^{n/2}exp\\{\\frac{1}{2\\sigma^2}(y-X\\beta)^T(y-X\\beta)\\}\n\\]\nWe can derive a joint conjugate prior\n\\[\n\\pi_0(\\beta,\\sigma^2) = \\pi_0(\\sigma^2)\\pi_0(\\beta|\\sigma^2)\n\\]\nwhere\n\\[\n\\pi_0(\\sigma^2) \\equiv InvGamma(a_0/2,b_0/2)\\\\\n\\pi_0(\\beta|\\sigma^2) \\equiv Normal_d(m_0,\\sigma^2M_0)\n\\]\nwhere \\(a_0,b_0,m_0,M_0\\) are user-defined constant hyperparameters. The joint posterior can hence be approximated with\n\\[\n\\pi_n(\\beta,\\sigma^2) \\propto \\mathcal{L}_n(\\beta,\\sigma^2)\\pi_0(\\beta,\\sigma^2) \\\\\n\\pi_0(\\sigma^2) = \\frac{(b_0/2)^{a_0/2}}{\\Gamma(a_0/2)}(\\frac{1}{\\sigma^2})^{a_0/2 +1}exp\\{-\\frac{b_0}{2\\sigma^2}\\}\\\\\n\\pi_0(\\beta|\\sigma^2) = (\\frac{1}{2\\pi\\sigma^2})^{d/2}\\frac{1}{|M_0|^{0.5}}exp\\{\\frac{1}{2\\sigma^2}(\\beta- m_0)^TM_0^{-1}(\\beta-m_0)\\}\n\\]\nWe can explore the exponents of the above posterior as a quadratic form.\nThe expression\n\\[\n(y - X\\beta)^T(y-X\\beta) + (\\beta - m_0)^TM_0^{-1}(\\beta-m_0) \\\\\n\\]\nwhich equates to \\((\\beta - m_n)^TM_n^{-1}(\\beta-m_n) + c_n\\) where we need to determine the expressions for \\(m_n, M_n,c_n\\).\n\nQuadratic term:\n\\(\\beta^TM_n^{-1}\\beta = \\beta^TX^TX\\beta + \\beta^T M_0^{-1}\\beta\\)\nand therefore\n\\(M_n^{-1} = X^TX + M_0^{-1}\\) –&gt;&gt; \\(M_n = (X^TX + M_0^{-1})^{-1}\\)\nLinear term:\n$\\beta^TM_n^{-1}m_n = \\beta^TX^Ty + \\beta^TM_0^{-1}m_0 $\nand therefore\n\\(m_n = M_n(X^Ty + M_0^{-1}m_0)\\)\n\\(= (X^TX + M_0^{-1})^{-1}(X^Ty +M_0^{-1}m_0)\\)\nConstant term:\n\\(m_n^TM_n^{-1}m_n + c_n = y^Ty + m_0^TM_0^{-1}m_0\\)\nand therefore\n\\(c_n = y^Ty + m_0^TM_0^{-1}m_0 - m_n^TM_n^{-1}m_n\\)\n\nGiven us the joint posterior (under proportionality) as\n\\[\n\\pi_n(\\beta,\\sigma^2) = (\\frac{1}{\\sigma^2})^{\\frac{(n+a_0+d)}{2}+1}\\exp\\{-\\frac{(c_n + b_0)}{2\\sigma^2}\\}\\exp\\{\\frac{1}{2\\sigma^2}(\\beta- m_n)^TM_n^{-1}(\\beta-m_n)\\}\n\\]\nWhich tells us that the conditional posterior and marginalizing the joint posterior over \\(\\beta\\) give us\n\\[\n\\pi_n(\\beta|\\sigma^2) \\equiv Normal_d(m_n,\\sigma^2M_n) \\\\\n\\pi_n(\\sigma^2) \\equiv InvGamma(a_n/2,b_n/2)\n\\]\nwhere \\(a_n = a_0 + n\\) and \\(b_n = b_0 + c_n\\).\nNote: see slides 218-221 for the marginal \\(\\beta\\) posterior where knowledge of the Inverse Gamma pdf will reveal that \\(\\pi_n(\\beta)\\) is a multivariate Student-t distribution.\nAssigning prior ignorance to \\(\\beta\\) (setting \\(M_0^{-1}\\) to 0) will lead to results that equate to what we would get from the maximum likelihood approach.\n\\[\nm_n \\rightarrow (X^TX)^{-1}X^Ty \\\\\nM_n \\rightarrow (X^TX)^{-1}\n\\]\nWe can alternatively use a g-prior: with hyperparameter \\(\\lambda &gt; 0\\).\n\\[\nM_0 = \\lambda^{-1}(X^TX)^{-1}\n\\]\nand hence\n\\[\nM_n = (1 + \\lambda)^{-1}(X^TX)^{-1}\n\\]\nIf, for the g-prior we have\n\\[\nm_0 = 0_d //\nM_0 = \\lambda I_d\n\\]\nthen we will have\n\\[\nm_n = (X^TX + \\lambda I_d)^{-1}X^Ty \\\\\nM_n = (X^TX + \\lambda I_d)^{-1}\n\\]\nwhich gives us the procedure for ridge regression.\nJeffrey’s prior for linear regression (see slide 228 for derivation):\n\\[\n\\pi_0(\\beta,\\sigma^2) \\propto (\\frac{1}{\\sigma^2})^{d/2 +1}\n\\]"
  },
  {
    "objectID": "BayesianModels.html#non-linear-regression",
    "href": "BayesianModels.html#non-linear-regression",
    "title": "9  Bayesian Modelling",
    "section": "9.3 Non-linear Regression",
    "text": "9.3 Non-linear Regression\n\n9.3.1 Generalized Linear Models"
  },
  {
    "objectID": "BayesianNonParametric.html#section",
    "href": "BayesianNonParametric.html#section",
    "title": "11  Non-parametrics",
    "section": "11.1 ",
    "text": "11.1 \n\n\n\n\n\n\nNote\n\n\n\nDid not get this far yet. Sorry!"
  },
  {
    "objectID": "PracticeProblems.html",
    "href": "PracticeProblems.html",
    "title": "12  Exercises",
    "section": "",
    "text": "12.0.1 Exercise 1\nPoisson model has mass function\n\\[\nf_Y(y;\\theta) = \\frac{\\theta^yexp\\{-\\theta\\}}{y!}\n\\]\nfor \\(y \\in \\mathbb{Z}^+\\)\nExplore the posterior density \\(\\pi_n(\\theta)\\) using two priors\n\n\\(Gamma(\\alpha_0,\\beta_0)\\)\nprior determined by the assumption that \\(\\phi = log\\theta \\sim Normal(\\eta_0,\\tau_0^2)\\) a priori. In other words, that the prior is a log-normal distribution\n\nWe have the following observations\n\n\n\ny\n0\n1\n2\n3\n4\n5\n6\n\n\nCount\n2\n6\n7\n16\n11\n6\n2\n\n\n\nAssuming that the likelihood is independent given \\(\\theta\\) (de Finetti), multiplying the joint likelihood by the Gamma prior gives us a Gamma posterior with observation-dependent parameters (due to gamma-poisson conjugacy - see Conjugate Distributions).\n\\[\n\\pi_n(\\theta) \\sim Gamma(\\alpha_0 + s_n,\\beta_0 + n) \\\\\ns_n = \\sum^n_{i=1}y_i\n\\]\nWe can now plot the posterior over all possible \\(\\theta \\in \\Theta\\).\n\na &lt;- 1\nb &lt;- 1\ny &lt;- c(rep(0,2),rep(1,6),rep(2,7),rep(3,16),rep(4,11),rep(5,6),rep(6,2))\ns_n &lt;- sum(y)\nn &lt;- length(y)\n\ntheta &lt;- seq(0,5,by = 0.01)\nll &lt;- sapply(theta,function(thet){\n  prod(dpois(y,thet))\n})\n#scalling likelihood to be visible in plot\nM &lt;- max(dgamma(theta,a+s_n,b+n))/max(ll)\nplot(theta,dgamma(theta,a+s_n,b+n),type = 'l',col = 'red',lwd = 2,\n     xlab = expression(theta),\n     ylab = 'Density')\nlines(theta,dgamma(theta,a,b),col = 'blue',lwd = 2)\nlines(theta,ll*M,col = 'orange',lwd = 2)\nlegend('topright',legend = c('Posterior',' Gamma Prior','Scaled Joint-Likelihood'),col = c('red','blue','orange'),lwd = 2)\n\n\n\n\n\neta &lt;- 0\ntau &lt;- 10\n#Can treat the joint-likelihood as a gamma dist with a = sn and b = n to avoid need for individual observations\nlnorm_pois &lt;- function(eta,tau,theta,sn,n){\n  calc &lt;- dgamma(theta,sn,n)*dnorm(log(theta),eta,tau)\n  return(calc)\n}\nnorm_const &lt;- integrate(lnorm_pois,eta = eta,tau =tau,sn = s_n,n = n,lower = 0, upper = 10)\n\nM &lt;- max(lnorm_pois(eta,tau,theta,s_n,n)/norm_const$value)/max(ll)\nS &lt;- max(lnorm_pois(eta,tau,theta,s_n,n))/max(dnorm(log(theta),eta,tau))\nplot(theta,lnorm_pois(eta,tau,theta,s_n,n)/norm_const$value,type = 'l',col = 'red',lwd = 2,\n     xlab = expression(theta),\n     ylab = 'Density')\nlines(theta,dnorm(log(theta),eta,tau)*S,col = 'blue',lwd = 2)\nlines(theta,ll*M,col = 'orange',lwd = 2)\nlegend('topright',legend = c('Posterior',' Log-Norm Prior','Scaled Joint-Likelihood'),col = c('red','blue','orange'),lwd = 2)\n\n\n\n\n\n\n12.0.2 Exercise 2"
  },
  {
    "objectID": "MoreModels.html",
    "href": "MoreModels.html",
    "title": "10  More Models!",
    "section": "",
    "text": "11 Multiparameter Models\nMost the examples we have explored have focused on modelling the posterior for single parameter models. That is, models where \\(\\theta\\) is a scalar as opposed to a vector of length \\(d\\). However, we know in practice that there are many phenomena which require multiple parameters . We previously saw this with the Gaussian distribution which has two parameters and in multivariate settings.\nAlthough it is necessary to formulate a multivariate generative process in terms of all of its parameters, it is not always the case that we are interested in the full posterior. Oftentimes, we may only be interested in the marginal posterior for specific parameters in the model (recall the multivariate Gaussian in chapter 8 ). You can imagine a scenario where we model an observation such as the temperature in a fridge as a Gaussian but may only be interested in the posterior of the mean.\nWe will therefore explore how marginalizing can allow us to separate our parameters of interest from nuisance parameters in our analysis of the posterior."
  },
  {
    "objectID": "MoreModels.html#marginal-posterior-distribution",
    "href": "MoreModels.html#marginal-posterior-distribution",
    "title": "10  More Models!",
    "section": "11.1 Marginal Posterior Distribution",
    "text": "11.1 Marginal Posterior Distribution\nLet’s imagine a scenario where we are building a model with parameters \\(\\theta = (\\theta_1,\\theta_2)\\). We may view \\(\\theta_2\\) as a nuisance parameter and need a way of expressing the posterior solely for \\(\\theta_1\\) , that is, \\(p(\\theta_1|y)\\). This can be achieves by integrating out \\(\\theta_2\\) as follows:\n\\[\np(\\theta_1|y) = \\int p(\\theta|y)d\\theta_2\n\\]\nWhich can be expanded as:\n\\[\np(\\theta_1|y) = \\int p(\\theta_1,\\theta_2|y)d\\theta_2 \\\\\n\\int p(\\theta_1|\\theta_2,y)p(\\theta_2|y)d\\theta_2\n\\]\nWhere \\(p(\\theta_1|\\theta_2,y)\\) is the conditional posterior distribution of \\(\\theta_1\\) given \\(\\theta_2\\) and \\(y\\)."
  },
  {
    "objectID": "MoreModels.html#example-1-normal-distribution-with-known-variance",
    "href": "MoreModels.html#example-1-normal-distribution-with-known-variance",
    "title": "10  More Models!",
    "section": "11.2 Example #1 Normal Distribution with known variance",
    "text": "11.2 Example #1 Normal Distribution with known variance\nThe normal distribution is a great example of a situation where a marginalized posterior may be of interest. In many cases, we may see the variance parameter in the normal distribution as a nuisance parameter, and may only be interested in the posterior distribution for the mean. We can therefore use it as a practical way of learning how to derive a marginal posterior.\nWe can start with a simpler setting where the variance is assumed to be known. We did something similar in a (________________) previous chapter where the mean was known and the variance unknown. These models provide a simple approach for deriving the conditional posteriors in cases where neither parameter is known."
  },
  {
    "objectID": "MoreModels.html#example-2",
    "href": "MoreModels.html#example-2",
    "title": "10  More Models!",
    "section": "11.3 Example #2",
    "text": "11.3 Example #2"
  },
  {
    "objectID": "PosteriorApproximation.html#sampling-methods",
    "href": "PosteriorApproximation.html#sampling-methods",
    "title": "8  Approximations",
    "section": "8.1 Sampling Methods",
    "text": "8.1 Sampling Methods\nTo approximate the posterior by simulation, we need to generate random samples from the posterior. This is trivial if the posterior is a known distribution with a derived simulation method. However, this becomes interesting when the posterior isn’t closed form. We can achieve this by making use of the unnormalized posterior density or any function \\(q(\\theta;y)\\) that is proportional to it.:\n\\[\np(\\theta|y) \\propto p(\\theta)p(y|\\theta) \\propto q(\\theta;y)\n\\]\nand generating random samples from this to approximate the posterior.\nTo generate the random samples, we can use rejection sampling or importance sampling for simple models. There are also many probabilistic programming tools available to automatically generate simulations.\n\n8.1.1 Grid Approximation\nThis method, also know as direct discrete approximation, works as follows:\n\nCreate an even-spaced grid \\(g_1 = a + i/2,…,g_m = b - i/2\\) where \\(a\\) and \\(b\\) are the lower and upper bounds of the parameter space of the posterior and \\(i\\) and \\(m\\) are the grid increments and number of grid points respectively.\nEvaluate the values of the unnormalized posterior density across the grid points \\(q(g_j;y)\\) and normalize them by their sum to estimate the posterior values:\n\\[\n\\hat{p_j} = \\frac{q(g_j;y)}{\\sum_{i = 1}^{m}q(g_i;y)}\\]\nFor every iteration \\(s = 1,…,S:\\)\n\nGenerate a sample for \\(\\theta_s\\) from a categorical distribution with \\(m\\) outcomes ($g_1,…,g_m$) with respective probabilities \\(\\hat{p}_1,...,\\hat{p}_m\\)\nAdd zero-centered uniformally distributed noise \\(\\epsilon\\) that has an interval length equal to the grid interval spacing such that:\n\\[\\hat{\\theta}_s = \\theta_s + \\epsilon\\]\nwhere \\(\\epsilon \\sim Uniform(-i/2,i/2)\\)\n\n\nThis generates a process similar to numerical integration, meaning that this approximation approach is limited to a finite interval.\nNote: I have been strictly using \\(\\theta\\) as the symbol for parameters across the different models used to make it clear what symbol represents the distribution parameter. In practice, different distributions make use of different symbols for their parameter. Examples are \\(\\alpha, \\beta, \\lambda\\) some of which were used in previous sections when defining the prior distributions.\n\n8.1.1.1 Example\nWe can demonstrate the usage of grid approximation with the Poisson-gamma model we explored previously. Simulation isn’t necessary here since the posterior is closed-form, but this will at least demonstrate how grid approximation is able to approximate it.\nRecall that the posterior of the Poisson-Gamma model had the following form:\n\\[\n\\Theta|Y \\sim Gamma(\\alpha + s_n,\\beta + n)\n\\]\n\ntrue_theta &lt;- 3\na &lt;- b &lt;- 1\nn &lt;- 5\nset.seed(123)\n\ny &lt;- rpois(n,true_theta)\n#Un-normalized Posterior (numerator for gamma distribution)\nq &lt;- function(theta,y,n,alpha,beta){\n  theta^(alpha + sum(y) - 1) * exp(-(n + beta)*theta)\n}\n\n#Explore theta on (0,25) --&gt; Need a fixed range \nlow_bound &lt;- 0\nhigh_bound &lt;- 20\n#Grid increment\ni &lt;- 0.01\ngrid &lt;- seq(low_bound + i/2, high_bound - i/2,by = i)\nn_sim &lt;- 10000\ngrid_n &lt;- length(grid)\ngrid_val &lt;- q(grid,y,n,a,b)\n#Normalize to create a proper categorical distribution\ngrid_norm &lt;- grid_val/sum(grid_val)\n\n## Simulate values\nsim_idx &lt;- sample(1:grid_n,n_sim,prob = grid_norm,replace = TRUE)\n\n#Noise parameter\ne &lt;- runif(n_sim,-i/2,i/2)\ntheta_sim &lt;- grid[sim_idx] + e\n\npar(mfrow = c(1,1),mar = c(4,4,1.5,1.5))\n## Visualize with histogram\nhist(theta_sim,col = 'orange',breaks = seq(0,8,0.25), probability = TRUE,\n     main = 'Grid Approximation using Poisson-Gamma Model',\n     xlab = expression(theta),xlim = c(0,6))\nlines(grid,dgamma(grid,a + sum(y),b + n),col = 'lightblue',lwd = 4)\nlegend('topright', legend = 'Ground Truth', col = 'lightblue',lwd = 4)\n\n\n\n## Visualize with density estimate\nplot(grid,dgamma(grid,a + sum(y),b + n), type = 'l',col = 'lightblue',lwd = 4, ylab = 'Density',\n     xlab = expression(theta),xlim = c(0,6))\nlines(density(theta_sim),col = 'orange',lwd = 4)\nlegend('topright', legend = c('Estimate','Ground Truth'), col = c('lightblue','orange'),lwd = 4)\n\n\n\n\nNote that we needed to explore \\(\\theta\\) on a fixed interval (0,25) to be able to approximate it. We can determine the plausible range of values to explore using the prior (if it is informative) or the likelihood. The key requirement is that we make sure the interval covers nearly all of the pmf for the distribution.\nWe can clearly see that the grid approximation has allowed us to correctly approximate the posterior without having to evaluate it analytically.\n\n\n8.1.1.2 Example #2 : Non-conjugate priors\nLet’s look at an example of a non-conjugate distribution, where the normalizing constant is such that the posterior is not a known distribution and is therefore not closed-form (intractable). A common example is using a log-normal prior for a Poisson likelihood. In essence, if our random variable is \\(X\\) and is normally distributed, we can use a reparameterization of the r.v, \\(Y = e^X\\) to get a log-normal distribution with the mean and variance being equal to the log of the mean and variance for \\(X\\) .\nOur model therefore uses the following distributions:\n\\[\nY \\sim Poisson(\\theta)\n\\]\n\\[\n\\theta \\sim Log  normal(\\mu,\\sigma^2)\n\\]\nThe pdf for the prior is as follows:\n\\[\np(\\theta) = \\frac{1}{\\theta \\sqrt{2\\pi\\sigma^2}}e^{-\\frac{(log\\theta - \\mu)^2}{2\\sigma^2}},\n\\]\nwhich gives the following un normalized posterior pdf\n\\[\np(\\theta|y) \\propto p(\\theta)p(y|\\theta)\n\\]\n\\[\n\\propto \\theta^{-1}\ne^{-\\frac{(log\\theta - \\mu)^2}{2\\sigma^2}}\\theta^{\\sum_{i = 1}^{n}y_i}e^{-n\\theta}\n\\]\n\\[\n\\propto\n\\theta^{\n{\\sum_{i = 1}^{n}y_i}-1}e^{-n\\theta\n- {\\frac{(log\\theta - \\mu)^2}{2\\sigma^2}}\n}\\]\nAnd a marginalized joint pdf (numerator) we cannot integrate over (intractable). We can now apply grid approximation! We will be reusing the distribution we create in the last example.\n\n#Approximating the distribution we generated in the previous example\nq &lt;- function(theta,y,n,mu,sigma2){\n  theta^(sum(y) - 1) * exp(-n*theta-(log(theta)-mu)^2 / (2*sigma2))\n}\nmu &lt;- 0\nsigma2 &lt;-1\n#Explore theta on (0,25) --&gt; Need a fixed range \nlow_bound &lt;- 0\nhigh_bound &lt;- 20\n#Grid increment\ni &lt;- 0.01\ngrid &lt;- seq(low_bound + i/2, high_bound - i/2,by = i)\ngrid_val &lt;- q(grid,y,n,mu,sigma2)\ngrid_norm &lt;- grid_val/sum(grid_val)\n\n## Simulate values\nsim_idx &lt;- sample(1:grid_n,n_sim,prob = grid_norm,replace = TRUE)\n#Noise parameter\ne &lt;- runif(n_sim,-i/2,i/2)\ntheta_sim &lt;- grid[sim_idx] + e\n\n## Visualize\npar(mfrow = c(1,1),mar = c(4,4,1.5,1.5))\n#plot 1\nhist(theta_sim,col = 'orange',breaks = seq(0,8,0.25), probability = TRUE,\n     main = 'Grid Approximation using Poisson-LogNormal',\n     xlab = expression(theta),xlim = c(0,8),ylim = c(0,0.75))\nlines(grid,dgamma(grid,a + sum(y),b + n),col = 'lightblue',lwd = 4)\nlegend('topright', legend = 'Ground Truth', col = 'lightblue',lwd = 4)\n\n\n\n##plot 2\n\nplot(grid,dgamma(grid,a + sum(y),b + n), type = 'l',col = 'lightblue',lwd = 4, ylab = 'Density',\n     xlab = expression(theta),xlim = c(0,8),ylim = c(0,0.75))\nlines(density(theta_sim),col = 'orange',lwd = 4)\nlegend('topright', legend = c('Estimate','Ground Truth'), col = c('lightblue','orange'),lwd = 4)\n\n\n\n\nWe can see that our approximation is close to the ground truth, but is slightly off. This could be seen as an indication that the selected prior may not be the best option (see result with gamma prior above)!"
  },
  {
    "objectID": "PosteriorApproximation.html#monte-carlo-methods",
    "href": "PosteriorApproximation.html#monte-carlo-methods",
    "title": "8  Approximations",
    "section": "8.2 Monte Carlo Methods",
    "text": "8.2 Monte Carlo Methods\nWe have now seen that a sufficient number of simulations can approximate the ground truth posterior. These simulations can also be used to calculate summary statistics such as the posterior mean, variance and credible intervals!\nIn general, computing integrals via simulations is referred to as Monte Carlo integration or the Monte Carlo method. These types of methods depend on the strong law of large numbers (SLL).\n\n8.2.0.1 Strong law of large numbers\nWhere we have a sequence of random variables \\(Y_i,…,Y_n\\) that are i.i.d. with a finite expected value \\(\\mu\\) , we have that almost surely (a.s.):\n\\[\n\\lim_{n\\to\\infty} \\frac{1}{n}\\sum_{i = 1}^{n}y_i = \\mu\n\\]\nWe are able to say almost surely as the above statement is said to converges to the result with a probability of one.\nSLL implies that the sample mean of an i.i.d sequence of random variables will always converge to the expected value of the underlying distribution with sufficient sample size. In the case of coin tosses, we would therefore expect that after enough trials, by SLL, that the expected value is 0.5 (assuming it is an unbiased coin!).\n\n\n8.2.0.2 Example: Monte Carlo Integration\nWe can revisit our example from the beginning of the chapter where we estimate \\(\\theta\\). By SLL, we expect that that average of the simulated values will converge to the true posterior mean of \\(\\theta\\) provided we simulate enough values. We can therefore approximate the posterior expectation using this mean. In this case, we know the posterior expectation of the Poisson-gamma model\n\\[\n\\mathbb{E}[\\Theta|Y = y] = \\frac{\\alpha_n}{\\beta_n} = \\frac{\\sum_{i = 1}^{n}Y_i +\\alpha}{n + \\beta}\n\\]\nso we can validate our approximation.\n\na_n &lt;- a + sum(y)\nb_n &lt;- b + n\n#True Expected Posterior\na_n/b_n\n\n[1] 3.333333\n\n#Simulated Posterior Mean\nmean(theta_sim)\n\n[1] 3.548937\n\ntheta_est &lt;- cumsum(theta_sim)/1:length(theta_sim)\nplot(1:length(theta_sim),theta_est,ylim = c(2.5,3.6),\n     ylab = expression(theta),\n     xlab = 'N')\nabline(h = a_n/b_n,col = 'red')\n\n\n\n\nThe same can be done to approximate the true posterior variance.\n\n#True Posterior variance\na_n/(b_n^2)\n\n[1] 0.5555556\n\n#Simulated Posterior variance\nvar(theta_sim)\n\n[1] 0.688096\n\n\nWe can also approximate posterior probabilities within an interval defined by an indicator \\(I_{(a,b)}(x)\\) where \\(I_{(a,b)}(x) = 1\\) if \\(x \\in (a.b)\\) and \\(I_{(a,b)}(x) = 0\\) otherwise:\n\n#True Posterior\npgamma(3,a_n,b_n,lower.tail = FALSE)\n\n[1] 0.6509161\n\n#Simulated Posterior\nmean(theta_sim &gt; 3)\n\n[1] 0.7323\n\n\nTherefore, we are also able to estimate quantiles:\n\nconf &lt;- 0.05\n\n#True 95% credible interval lower bound\nqgamma(conf/2,a_n,b_n)\n\n[1] 2.036087\n\n#Estimated 95% credible interval lower bound\nquantile(theta_sim,conf/2)\n\n    2.5% \n2.127166 \n\n\n\n#True 95% credible interval upper bound\nqgamma(1- conf/2,a_n,b_n)\n\n[1] 4.945142\n\n#Estimated 95% credible interval upper bound\nquantile(theta_sim,1- conf/2)\n\n   97.5% \n5.346038 \n\n\n\n\n8.2.1 Importance Sampling\nIn cases where we can’t generate samples from the target distribution, Monte Carlo methods allow us to generate viable samples by means of a surrogate distribution. In essence, we aim to identify a pdf \\(f_0\\) for which sampling is possible to then estimate the features of interest of our target distribution’s pdf \\(f\\). For this to work, it is imperative that the selected pdf has the support for the same domain as the target pdf. In the case of determining an expected value we get:\n\\[\n\\int{g(x)f(x)}dx = \\int{g(x)f(x)\\frac{f_0(x)}{f_0(x)}}dx =  \\int{\\frac{g(x)f(x)}{f_0(x)}f_0(x)}dx\n\\]\nWhich gives us\n\\[\n\\mathbb{E}_f[g(X)] = \\mathbb{E}_{f_0}[\\frac{g(X)f(X)}{f_0(X)}]\n\\]\nUsing the SLL as discussed previously, we can then use the estimator \\(\\hat{I}^{(f_0)}_N(g)\\) to get the expected value over the target pdf:\n\\[\\hat{I}^{(f_0)}_N(g) = \\frac{1}{N}\\sum_{i=1}^{N}\\frac{g(X_i)f(X_i)}{f_0(X_i)}\\]\nIn practice, \\(\\hat{I}^{(f_0)}_N(g)\\) is known as the importance sampling estimator. To emphasize the notion of importance, we can rearrange the equation as follows:\n\\[\\hat{I}^{(f_0)}_N(g) = \\frac{1}{N}\\sum_{i=1}^{N}\\frac{f(X_i)}{f_0(X_i)}g(X_i) = \\sum_{i=1}^{N}w_0(X_i)g(X_i)  \\]\nWhere \\(w_0(X_i)\\) represents the importance sampling weight. We would choose to use the importance sampler estimator over the standard estimator in cases where its variance is less than that of the standard estimator.\nLet’s look at an example. Let us say that we want to get the following expectation over \\(f(x)\\)\n\\[\nE_f[g(x)]\n\\]\nWhere we know that \\(f(x)\\) is the pdf of a Gamma distribution \\(X \\sim Gamma(2,3)\\). We aim to compute \\(\\mathbb{E}[X^3]\\). As we know the ground truth, we know that the expected value for the random variable of a gamma function is:\n\\[\\mathbb{E}[X^r] = \\frac{1}{\\beta^r}\\frac{\\Gamma(\\alpha + r)}{\\Gamma(\\alpha)}\\]\n\na &lt;- 2\nb &lt;- 3\nr &lt;- 4\ntrue_expec &lt;- gamma(a + r)/((b^r) *gamma(a))\nx &lt;- seq(0,5,by = 0.01)\ny &lt;- x^r\npar(mar=c(3,3,2,2))\nplot(x,y*dgamma(x,2,3),type = 'l',col = 'red',\n     xlab = 'X',ylab = '',ylim = c(0,1.2))\nlines(x,dgamma(x,2,3),col = 'blue')\nlegend('topright',legend = c('g(x)f(x)','f(x)'),col = c('red','blue'))\n\n\n\n\nWe can now evaluate importance sampling estimators that use a series of \\(f_0\\) functions:\n\n\\(Gamma(2,3)\\) - Standard MC Estimator\n\\(Gamma(6,3)\\)\n\\(Student(5)\\) centered around \\(x = 2\\)\n\\(Normal(3,2)\\)\n\\(Gamma(2,6)\\)\n\nDoing this for multiple runs will help us determine the variance of each estimator.\n\nruns &lt;- 1000\nsamples &lt;- 10000\nestimates &lt;- matrix(0,nrow = runs,ncol = 5)\nfor (run in 1:runs){\n  X1 &lt;- rgamma(samples,a,b)\n  X2 &lt;- rgamma(samples,6,3)\n  X3 &lt;- rt(samples,5) + 2\n  X4 &lt;- rnorm(samples,3,2)\n  X5 &lt;- rgamma(samples,5,2)\n  Y1 &lt;- (X1^r)\n  Y2 &lt;- (X2^r)*dgamma(X2,a,b)/dgamma(X2,6,3)\n  Y3 &lt;- (X3^r)*dgamma(X3,a,b)/dt(X3-2,5)\n  Y4 &lt;- (X4^r)*dgamma(X4,a,b)/dnorm(X4,3,2)\n  Y5 &lt;- (X5^r)*dgamma(X5,a,b)/dgamma(X5,5,2)\n  estimates[run,] &lt;- c(mean(Y1),mean(Y2),\n                       mean(Y3),mean(Y4),mean(Y5))\n}\npar(mar=c(3,3,2,2))\nboxplot(estimates)\ntitle('Comparison of IS Estimators over 1000 Replicates')\nabline(h = true_expec,col = 'red')\n\n\n\n\nAs we can see, the standard MC estimator actually has a lot more uncertainty than the IS approaches. We notice that the estimators using a gamma pdf work quite well.\n\n\n8.2.2 Optimal Importance Sampling\nWhen choosing \\(f_0(x)\\), we want to ensure that the variance is finite so that our confidence on the estimate is quantifiable. The estimator’s variance is finite if and only if\n\\[\n\\frac{g(x)f(x)}{f_0(x)}\n\\]\nhas finite variance. In other words, it is finite if\n\\[\n\\mathbb{E}_{f_0}[\\{\\frac{g(x)f(x)}{f_0(x)}\\}^2] = \\int_{-\\infty}^\\infty\\{\\frac{g(x)f(x)}{f_0(x)}\\}^2f_0(x)dx\n\\]\nis finite. We therefore conclude that an optimal choice for \\(f_0\\) occurs when\\(f_0(x) \\propto |g(x)|f(x)\\)\nWhen the ratio \\(\\frac{f(x)}{f_0(x)}\\) has unbounded support, the variance of our estimator will not be finite. We therefore need a way to ensure that the ratio remains bounded at the tails for \\(f\\) with unbounded support. In practice, we find that\n\\[\n\\lim_{n\\to\\infty}\\frac{1}{N}\\sum_{i = 1}^N\\frac{f(X_i)}{f_0(X_i)} = \\int_{-\\infty}^\\infty\\frac{f(X_i)}{f_0(x)}f_0(x)dx = 1\n\\]\nallow us to derive the following estimator\n\\[\n\\tilde{I}_N^{(f_0)}(g) = \\frac{\\sum_{i=1}^{N}\\frac{g(X_i)f(X_i)}{f_0(X_i)}}{\\sum_{i = 1}^N\\frac{f(X_i)}{f_0(X_i)}}\\xrightarrow{\\text{a.s.}} \\mathbb{E}_f[g(X)]\\]\nWhich, if it exists, may have a smaller variance than the variance we saw earlier.\n-Note: consider adding something about harmonic mean derivation\n\n\n8.2.3 Rejection Sampling\n\ntrue_dist &lt;- function(x){   return(dnorm(x,30,10)+ dnorm(x,80,20))  } \nestim_dist &lt;- function(x){   return(dnorm(x,50,30)) } \nx &lt;- seq(-50,150) \nc &lt;- max(true_dist(x)/estim_dist(x)) \nplot(x,true_dist(x),type = 'l',ylim = c(0,0.06)) \nlines(x,c*estim_dist(x)) \n\n\n\n\n\nrej_sample &lt;- function(n,c){   \n  x &lt;- rnorm(n,50,30)  \n  k &lt;- runif(n)   \n  return(x[true_dist(x)/(c*estim_dist(x)) &gt; k]) }\nrej_x &lt;- rej_sample(100000,c) \nhist(rej_x,freq = F, ylim = c(0,0.02)) \nlines(x = density(x = rej_x))\n\n\n\n\n\n\n8.2.4 Sampling Importance Resampling\nDespite being similar, there is a key difference between importance and rejection sampling:\n\nImportance sampling focuses on approximating numerical integration with respect to \\(f(x)\\)\nRejection sampling focuses on generating i.i.d samples from \\(f(x)\\)\n\nBased on these differences, we can consider Sampling Importance Resampling (SIR) which can sample from density \\(f(x)\\) by re-weighting and then resampling samples from \\(f_0(x)\\):\n\nGenerate samples \\(x_1,…,x_n\\) from \\(f_0\\)\nCalculate re-normalized weights \\(w_1,…,w_n\\) \\[w_i = \\frac{\\frac{f(X_i)}{f_0(X_i)}}{\\sum_{j = 1}^N\\frac{f(X_j)}{f_0(X_j)}} \\]\nGenerate new samples (resample) from the discrete distribution of the samples \\(x\\) using the weights \\(w\\) as the probability masses.\n\nNote: make an example"
  },
  {
    "objectID": "PosteriorApproximation.html#markov-chain-monte-carlo-mcmc-methods",
    "href": "PosteriorApproximation.html#markov-chain-monte-carlo-mcmc-methods",
    "title": "8  Approximations",
    "section": "8.3 Markov Chain Monte Carlo (MCMC) Methods",
    "text": "8.3 Markov Chain Monte Carlo (MCMC) Methods\nGrid approximation worked well in our 1-dimensional example. However, this is not always the case when we begin to work with high-dimensional data. To cover the parameter space we defined in our example, we generated a grid with 2000 points. However, if we had additional dimensions each with 2000 points, we would find ourselves working with \\(2000^d\\) grid points. This becomes quite impractical in what has been a very simple example. Clearly, grid approximation will not be viable for higher dimensions.\nAs a result, most sampling approaches for more complex models usually make use of Markov Chain Monte Carlo (MCMC) methods. This family of methods focuses on iterative sampling from a markov chain for which the distribution at convergence matches the target distribution. In our case, this is the posterior distribution.\n\n8.3.0.1 Markov Chain\nA Markov chain is a sequence of random variables \\(X_1,X_2,…\\) that have a Markov property. That is, for any discrete time point \\(i\\) :\n\\[P(X_{i+1} | X_i,X_{i-1},...,X_0) = P(X_{i+1} | X_i)\\]\nThat is to say, for any discrete time point $i$, the state of the random variable at the next time point, \\(X_{i+1}\\) depends only on the current state \\(X_i\\) . The set of all possible values or states that the random variables can take is known as the state space \\(S\\).\n\n\n8.3.0.2 MCMC Sampling\nSimplistic sampling methods such as the ones we saw previously generate i.i.d. samples from the target distribution. However, the values we sample from the state space S (\\(\\theta_1,…,\\theta_S\\)) by an MCMC method experience high auto-correlation. That is, the value sampled at the next time point will be similar to the current value. Although this would be an issue for a small sample size, MCMC overcomes this by generating a large number of samples and using the full sample to approximate the posterior distribution. MCMC algorithms converge when they reach the stationary distribution. That is, the stationary distribution is a distribution \\(\\pi(x)\\) achieved at a state where for all \\(i \\in \\mathbb{Z}^+\\) :\n\\[\nP(X_i = k) = \\pi(k)\n\\]\nwhich implies that the next state of the random variable is the same as the current state. Once it has reached convergence, sampling from the stationary distribution will eventually lead to an approximation of the true posterior. Determining when convergence has been reached is difficult to determine. There are different diagnostics available to explore this, one of which is to running different MCMC instances with different initializations and see if they reach a similar distribution.\nAs the first iterations of MCMC sampling do not represent the posterior yet, they are usually discarded. The number of iterations that are discarded is up to the user and is referred to as the burn-in period. Markov chains that were designed to estimate target posterior distributions are known as MCMC samplers. The two most popular examples are the Gibbs sampler and the Metropolis-Hastings sampler. The Gibbs sampler is actually a special case of the MH sampler.\n\n\n8.3.0.3 Example: Gibbs sampler\nThe Gibbs sampler works by incrementally updating each of the components of the parameter vector \\(\\theta\\). We can assume that the parameter vector is multi-dimensional where for each component \\(\\theta_j\\) , the sampler generates a value using the conditional posterior distribution of the component given all the other components.\n\\[\np(\\theta_j | \\theta_{-j},y)\n\\]\nNotice that the current component is omitted from the parameter vector being used as prior information!\nWe can explore the concept with a two-dimensional example. We assume that we have one observation \\((y_1,y_2) = (0,0)\\) which comes from a two-dimensional normal distribution \\(N(\\mu,\\Sigma_0)\\) , where the parameter we are interested in is the mean vector \\(\\mu = (\\mu_1,\\mu_2)\\) and the covariance matrix\n\\[\\Sigma_0 =\n\\begin{bmatrix}\n1 & \\rho \\\\\n\\rho & 1 \\\\\n\\end{bmatrix} \\] is assumed to be known and constant with \\(\\rho = -0.7\\) . We can also assume that we are using an improper uniform prior \\(p(\\mu) \\propto 1\\). The posterior is now a two-dimensional normal distribution \\(N(\\mu,\\Sigma_0)\\). Note that for now we will not consider the inference of the posterior. Normally we could generate samples from pre-existing implementations but we will look at making a rudimentary implementation of a Gibbs sampler.\nWe have the following conditional posteriors for each mean of interest:\n\\[\n\\mu_1|\\mu_2,Y \\sim N(y_1 + \\rho(\\mu_2-y_2),1 - \\rho^2)\n\\]\n\\[\n\\mu_2|\\mu_1,Y \\sim N(y_2 + \\rho(\\mu_1-y_1),1 - \\rho^2)\n\\]\n\ny &lt;- c(0,0)\nrho &lt;- -0.7\nset.seed(111)\nmu1_sample &lt;- function(y, rho, mu2) rnorm(1, y[1] + rho * (mu2-y[2]), sqrt(1-rho^2))\nmu2_sample &lt;- function(y, rho, mu1) rnorm(1, y[2] + rho * (mu1-y[1]), sqrt(1-rho^2))\n\nn_sim &lt;- 100\nmu1 &lt;- mu2 &lt;- numeric(n_sim)\nmu1[1] &lt;- mu2[1] &lt;- 2\nfor (i in 2:n_sim){\n  mu1[i] &lt;- mu1_sample(y,rho,mu2[i-1])\n  mu2[i] &lt;- mu2_sample(y,rho,mu1[i])\n}\n\nlibrary(ggplot2)\n\nWarning: package 'ggplot2' was built under R version 4.2.3\n\nlibrary(gganimate)\n\nWarning: package 'gganimate' was built under R version 4.2.3\n\ngibbs_df &lt;- data.frame(mu1,mu2, iteration = 1:n_sim)\ngibbs_df$start &lt;- '-'\ngibbs_df$start[1] &lt;- \"Start\"\n\nggplot(gibbs_df,aes(mu1,mu2,shape = start)) + \n  geom_point() + geom_path(aes(group = 'all'))  + \n  theme(panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank(),\npanel.background = element_blank(),\naxis.line = element_line(colour = \"black\"))\n\n\n\nggplot(gibbs_df,aes(mu1,mu2,shape = start)) + \n  geom_path( aes(group = 'all'))  + \n  theme(panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank(),\npanel.background = element_blank(),\naxis.line = element_line(colour = \"black\")) +\n  labs(title = 'Iteration: {frame_along}') +\n  transition_reveal(along = iteration)\n\n`geom_path()`: Each group consists of only one observation.\nℹ Do you need to adjust the group aesthetic?\n`geom_path()`: Each group consists of only one observation.\nℹ Do you need to adjust the group aesthetic?\n\n\n\n\nanim_save('mcmc.gif', animation = last_animation(), path = NULL)\n\nWe can compare our results with the true posterior to see how well we approximated the distribution.\n\ncovar &lt;- matrix(c(1,rho,rho,1),ncol =2)\n\nX &lt;- MASS::mvrnorm(n_sim,y,covar)\n\npar(mfrow = c(1,2))\n plot(mu1, mu2, pch = 16, col = 'orange',\n       xlim = c(-4,4), ylim = c(-4,4), asp = 1, xlab = expression(mu[1]), \n       ylab = expression(mu[2]),\n      main = 'MCMC')\n  plot(X[,1], X[,2], pch = 16, col = 'orange',\n       xlim = c(-4,4), ylim = c(-4,4), asp = 1, xlab = expression(mu[1]), \n       ylab = expression(mu[2]),\n       main = 'True Posterior')\n\n\n\n\nAs we can see, despite starting with estimates well off of the true values, the Gibbs sampler was able to reach a distribution that is quite similar to the true posterior.\n\n\n8.3.1 Metropolis-Hastings\nAn alternative method for MCMC sampling is the Metropolis-Hastings algorithm. In this approach, we iteratively propose a random sample of the random variable as the next state \\(z\\) given our current state \\(x\\). This is reffered to as a transition kernel \\(Q(z|x)\\) which is often simply denoted as a normal distribution with the current state \\(x\\) as the mean and a standard deviation equal to 1. By defining criteria that allows us to accept or reject the proposed new sample, we are able to explore the domain of possible values generated by the target distribution through what is a random walk (think Brownian motion). The criteria for acceptance is known as the acceptance probability and is calculated as follows:\n\\[\nA = min(1,\\frac{\\pi(z)Q(x|z)}{\\pi(x)Q(z|x)}\n\\]\nIn the case of a transition kernel which has a random walk behavior as the one described above, we find that \\(Q(z|x) = Q(x|z)\\) allowing us to exclude both from the calculation of the acceptance probability.\nTo achieve this in our multi-dimensional setting, we would make use of the following steps:\n\nInitialize the state \\(X_1 = (\\mu_1,\\mu_2)\\)\nFor t steps:\n\nsample \\(z\\) from the transition kernel \\(Q(z|x)\\)\nCompute the acceptance probability\nAccept or reject the proposed state\n\n\nWe can apply this to our previous distribution ( \\(\\mu_1= \\mu_2 =0\\) ) to see how well we can replicate its sampling.\n\nt &lt;- 10000\nx &lt;- matrix(0,nrow = 2,ncol = t)\nx[,1] &lt;- c(-2,2)\nz &lt;- c(0,0)\nrho &lt;- -0.7\ncovar &lt;- matrix(c(1,rho,rho,1),ncol =2)\nfor (i in 2:t){\n  z[1] &lt;- rnorm(1,x[1,i-1])\n  z[2] &lt;- rnorm(1,x[2,i-1])\n  A &lt;- min(1,mvtnorm::dmvnorm(z,mean = rep(0,2),sigma = covar))\n  if (runif(1) &lt; A){\n    x[,i] &lt;- z\n  } else {\n    x[,i] &lt;- x[,i-1]\n  }\n}\nx &lt;- as.data.frame(t(x))\nggplot(x,aes(V1,V2)) + geom_point()  + \n  theme(panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank(),\npanel.background = element_blank(),\naxis.line = element_line(colour = \"black\")) +\n  labs(title = 'Metropolis-Hastings Algorithm Samples',\n       x = expression(mu[1]),\n       y = expression(mu[2]))\n\n\n\n\nOnce again, we see that the MCMC sampler is generating samples in a similar fashion to the target distribution!\n\n\n8.3.2 Probabilistic Programming - Incomplete"
  },
  {
    "objectID": "PosteriorApproximation.html#sampling-from-the-posterior-predictive-distribution---needs-revision",
    "href": "PosteriorApproximation.html#sampling-from-the-posterior-predictive-distribution---needs-revision",
    "title": "8  Approximations",
    "section": "8.4 Sampling from the Posterior Predictive Distribution - NEEDS REVISION",
    "text": "8.4 Sampling from the Posterior Predictive Distribution - NEEDS REVISION\nOnce we have determined the posterior distribution, sampling from the posterior predictive distribution (recall chapter 3) becomes simple.\nAssume we wan to predict probabilities for new the observation \\(Y'\\) from the generative process derived from the original observations \\(Y\\) . We can assumed we generated \\(s\\) parameter values \\(\\theta_1,...,\\theta_s\\) from the posterior distribution \\(p(\\theta|y)\\). With these sampled values we can then sample \\(Y'\\) from as follows:\n\nFor all \\(s = 1,..,S\\):\n\nSample \\(Y'_s \\sim p(y'|\\theta_s)\\)\n\n\nWe therefore sample a new observation for each sampled parameter value. The empirical distribution of this sampling procedure can be used to approximate the posterior predictive distribution.\n\\[\np(y'|y) = \\int p(y'|\\theta)p(\\theta|y)d\\theta\n\\]\n\n8.4.1 Example #1\nLet’s create a simple example using a beta-binomial model used in chapter 3. Recall that the beta-binomial conjugacy meant that the sum of all observation of \\(Y\\), \\(s_n\\) was all that was needed to calculate the joint likelihood and the resulting posterior.\n\na &lt;- 1\nb &lt;- 3\ntheta &lt;- 0.25\nn &lt;- 250\nsn  &lt;- theta*n\ntheta_seq &lt;- seq(0,1,by = 0.01)\nplot(theta_seq,dbeta(theta_seq,a + sn,n-sn+b), type = 'l',\n     col = 'red',\n     xlab = expression(theta),\n     ylab = 'Density',\n     main = 'Posterior Distribution for Beta-Binomial Model')\n\n\n\n\nAgain, we see that the posterior heavily favors the true parameter value. Let us now use this to sample the posterior predictive. We can do this in two ways:\n\nPlot the individual y values after sampling \\(S\\) times.\nPlot the distribution of the summary statistics after sampling \\(S\\) observations \\(x\\) times.\n\nBecause the binomial distribution is discrete, we can plot the posterior predictive distribution in terms of the sufficient statistics to get more interpretable results.\n\n#Sample groups of observations\ntrials &lt;- 500\ns &lt;- 100\nsn_post &lt;- rep(0,trials)\ntrue_post &lt;- dbinom(1:s,s,theta)\nfor (i in 1:trials){\n  sn_post[i] &lt;- rbinom(1,s,rbeta(s,a +sn,n-sn+b))\n}\n#Normaliz\nhist(sn_post,\n     xlab = expression(s_n),\n     main = 'Posterior Predictive Distribution')\nabline(v = theta*s,\n     col = 'red')\n\n\n\nwhich.max(true_post)/s\n\n[1] 0.25"
  },
  {
    "objectID": "MoreModels.html#multiparameter-models---incomplete",
    "href": "MoreModels.html#multiparameter-models---incomplete",
    "title": "10  More Models!",
    "section": "10.1 Multiparameter Models - INCOMPLETE",
    "text": "10.1 Multiparameter Models - INCOMPLETE\nMost the examples we have explored have focused on modelling the posterior for single parameter models. That is, models where \\(\\theta\\) is a scalar as opposed to a vector of length \\(d\\). However, we know in practice that there are many phenomena which require multiple parameters . We previously saw this with the Gaussian distribution which has two parameters and in multivariate settings.\nAlthough it is necessary to formulate a multivariate generative process in terms of all of its parameters, it is not always the case that we are interested in the full posterior. Oftentimes, we may only be interested in the marginal posterior for specific parameters in the model (recall the multivariate Gaussian in chapter 8 ). You can imagine a scenario where we model an observation such as the temperature in a fridge as a Gaussian but may only be interested in the posterior of the mean.\nWe will therefore explore how marginalizing can allow us to separate our parameters of interest from nuisance parameters in our analysis of the posterior.\n\n10.1.1 Marginal Posterior Distribution\nLet’s imagine a scenario where we are building a model with parameters \\(\\theta = (\\theta_1,\\theta_2)\\). We may view \\(\\theta_2\\) as a nuisance parameter and need a way of expressing the posterior solely for \\(\\theta_1\\) , that is, \\(p(\\theta_1|y)\\). This can be achieves by integrating out \\(\\theta_2\\) as follows:\n\\[\np(\\theta_1|y) = \\int p(\\theta|y)d\\theta_2\n\\]\nWhich can be expanded as:\n\\[\np(\\theta_1|y) = \\int p(\\theta_1,\\theta_2|y)d\\theta_2 \\\\\n\\int p(\\theta_1|\\theta_2,y)p(\\theta_2|y)d\\theta_2\n\\]\nWhere \\(p(\\theta_1|\\theta_2,y)\\) is the conditional posterior distribution of \\(\\theta_1\\) given \\(\\theta_2\\) and \\(y\\).\n\n\n10.1.2 Example #1 Normal Distribution with known variance\nThe normal distribution is a great example of a situation where a marginalized posterior may be of interest. In many cases, we may see the variance parameter in the normal distribution as a nuisance parameter, and may only be interested in the posterior distribution for the mean. We can therefore use it as a practical way of learning how to derive a marginal posterior.\nWe can start with a simpler setting where the variance is assumed to be known. We did something similar in a (________________) previous chapter where the mean was known and the variance unknown. These models provide a simple approach for deriving the conditional posteriors in cases where neither parameter is known.\n\n10.1.2.1 One observation\nIn a setting with one observation \\(Y\\) coming from a normal distribution with known variance \\(\\sigma_0^2 &gt; 0\\) and unknown mean \\(\\theta\\). The conjugate prior in this setting is another normal:\n\\[\nY \\sim Normal(\\theta,\\sigma_0^2) \\\\\n\\theta \\sim Normal(\\mu_0,\\tau_0^2)\n\\]\nWith this formulation we have the following likelihood and prior pdfs:\n\\[\np(y|\\theta) = \\frac{1}{\\sqrt{2\\pi\\sigma_0^2}} \\exp\\left(-\\frac{(y - \\theta)^2}{2\\sigma_0^2}\\right) \\propto \\exp\\left(-\\frac{(y - \\theta)^2}{2\\sigma_0^2}\\right) \\\\\n\\]\n\\[\np(\\theta)  = \\frac{1}{\\sqrt{2\\pi\\tau_0^2}} \\exp\\left(-\\frac{(\\theta - \\mu_0)^2}{2\\tau_0^2}\\right) \\propto \\exp\\left(-\\frac{(\\theta - \\mu_0)^2}{2\\tau_0^2}\\right)\n\\]\nwith the unnormalized posterior being:\n\\[\np(\\theta|y)\\propto \\exp\\left(-\\frac{(y - \\theta)^2}{2\\sigma_0^2}-\\frac{(\\theta - \\mu_0)^2}{2\\tau_0^2} \\right) \\\\\n\\propto \\exp\\left(-\\frac{\\tau_0^2(y - \\theta)^2 + \\sigma_0^2(\\theta - \\mu_0)^2}{2\\tau_0^2\\sigma_0^2}\\right) \\\\\n\\propto \\exp\\left(\\frac{\\tau_0^2(\\theta^2-2y\\theta) + \\sigma_0^2(\\theta^2 -2\\mu_0\\theta)}{2\\tau_0^2\\sigma_0^2}\\right) \\\\\n\\propto \\exp\\left(-\\frac{(\\sigma_0^2 + \\tau_0^2)\\theta^2 -2(\\sigma_0^2\\mu_0 + \\tau_0^2y)\\theta}{2\\tau_0^2\\sigma_0^2}\\right) \\\\\n\\propto \\exp\\left(-\\frac{\\theta^2 -2\\mu_1\\theta}{2\\tau_1^2}\\right)\n\\]\nWhere\n\\[\n\\mu_1 = \\frac{(\\sigma_0^2\\mu_0 + \\tau_0^2y)}{(\\sigma_0^2 + \\tau_0^2)} \\\\\n\\tau_1 = \\frac{\\tau_0^2\\sigma_0^2}{(\\sigma_0^2 + \\tau_0^2)}\n\\]\nWhich returns a posterior in the form of a normal distribution with mean \\(\\mu_1\\) and variance \\(\\tau_1^2\\). It is also possible to write the parameters of the posterior in terms of precision. Precision is the inverse of the posterior variance, and can be expressed as the sum of the prior and sampling precision.\n\\[\n\\frac{1}{\\tau_1^2} = \\frac{1}{\\tau_0^2} + \\frac{1}{\\sigma_0^2}\n\\]\nSimilarly, the posterior mean can be expressed as a convex combination (sum of prior and sampling means weighted by the proportion of their proportional precision) of the prior mean and the observed data.\n\\[\n\\mu_1 = \\frac{\\frac{1}{\\tau_0^2}\\mu_0 + \\frac{1}{\\sigma_0^2}y}{\\frac{1}{\\tau_0^2} + \\frac{1}{\\sigma_0^2}}\n\\]\n\n\n10.1.2.2 Many observations\nMaking the derivation for multiple observations works in a similar fashion, except that we now use a joint-likelihood:\n\\[\np(y|\\theta) = \\prod_{i = 1}^np(y_i|\\theta)\n\\]\nTo make the calculation simple, we can take advantage of the fact that the mean of the normally distributed variables follows a normal distribution too:\n\\[\n\\overline{Y} \\sim Normal(\\theta,\\sigma^2/n)\n\\]\nWe can then use the solution for the single observation example (and now you see why we started with that :) ) to derive the posterior parameters as:\n\\[\n\\mu_n = \\frac{\\frac{1}{\\tau_0^2}\\mu_0 + \\frac{n}{\\sigma_0^2}\\overline{y}}{\\frac{1}{\\tau_0^2} + \\frac{n}{\\sigma_0^2}}\n\\]\nand\n\\[\n\\frac{1}{\\tau_n^2} = \\frac{1}{\\tau_0^2} + \\frac{n}{\\sigma_0^2}\n\\]\nNow if we had not derive the single observation posterior first, we could have also done this the hard way. But since you’re already here, I say why not both?\nWe start with expanding the joint likelihood:\n\\[\np(y|\\theta) = \\prod_{i = 1}^np(y_i|\\theta) = \\left(\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\right)^n\\exp\\left(\\frac{\\sum_{i=1}^n(y_i - \\theta)^2}{2\\sigma^2}\\right)\n\\]\n\\[\n\\propto  \\exp\\left(-\\frac{\\sum_{i=1}^n(y_i - \\theta)^2}{2\\sigma^2}\\right)\n\\]\nAt first glance it may look daunting to find a closed form solution to our problem and identifying a sufficient statistic seems unlikely. However, we can rely on the achievements of our predecessors to get through this. Here, we can use sum of squares decomposition:\n\\[\\sum_{i=1}^n(y_i - \\theta)^2 = n(\\overline{y} - \\theta)^2 + \\sum_{i=1}^n(y_i - \\overline{y})^2\\]\nWhere \\(\\overline{y}\\) is the mean of the observed values. We now have the following:\n\\[\np(y|\\theta)\\propto  \\exp\\left(-\\frac{n(\\overline{y} - \\theta)^2 + \\sum_{i=1}^n(y_i - \\overline{y})^2}{2\\sigma^2}\\right)\n\\]\n\\[\n\\propto  \\exp\\left(-\\frac{n(\\overline{y} - \\theta)^2}{2\\sigma^2}\\right)\n\\]\nAs the second component does not depend on \\(\\theta\\). All of a sudden we now have a pdf that depends only on a sufficient statistic! We can now multiply this by the prior and see that it matches the form of the posterior we saw earlier:\n\\[\np(\\theta|y) \\propto  \\exp\\left(\\frac{n(\\overline{y} - \\theta)^2}{2\\sigma^2}\\right)\\exp\\left(-\\frac{(\\theta - \\mu_0)^2}{2\\tau_0^2}\\right)\n\\]\n\\[\n= \\exp\\left(-\\frac{n\\tau_0^2(\\overline{y} - \\theta)^2 + \\sigma_0^2(\\theta - \\mu_0)^2}{2\\tau_0^2\\sigma_0^2}\\right)\n\\]\nFrom here we can notice the similar form of the posterior from the single observation example, with the only exception being that there is also the variable \\(n\\).\n\n\n\n10.1.3 Example #2 Non-informative prior\nWe assumed that the mean followed a normal distribution but what if this is not the case? Recall that in chapter 2 we looked at different choices of priors. In particular, we considered the non-informative prior for cases where we don’t have intuition on the shape of the prior.\nImagine a situation where both the mean and variance are unknown. Determining the joint prior may be more difficult, so the use of a non-informative prior may be a better choice.\nLet’s use the following improper prior:\n\\[\np(\\theta,\\sigma^2) \\propto \\frac{1}{\\sigma^2}\n\\]\nAs the variance is unknown, our likelihood from example 1 will look as follows:\n\\[\np(y|\\theta,\\sigma^2) \\propto \\frac{1}{\\sigma^n}\\exp\\left(-\\frac{n(\\overline{y} - \\theta)^2 + \\sum_{i=1}^n(y_i - \\overline{y})^2}{2\\sigma^2}\\right)\n\\]\n\\[\n= \\frac{1}{\\sigma^n}\\exp\\left(-\\frac{(n-1)s^2 +n(\\overline{y} - \\theta)^2}{2\\sigma^2}\\right)\n\\]\nNotice again that we have to retain terms with \\(\\sigma\\) as they are no longer constant. Here, we had to retain the sum of square error between the sample mean and observations, but can express it as another sufficient statistic, \\(s^2\\) which represents the sample variance.\n\\[\ns^2 = \\frac{\\sum_{i=1}^n(y_i - \\overline{y})^2}{n-1}\n\\]\nThis then gives us the following un-normalized posterior:\n\\[\np(\\theta,\\sigma^2|y) \\propto \\frac{1}{\\sigma^{(n+2)}}\\exp\\left(-\\frac{(n-1)s^2 + n(\\overline{y} - \\theta)^2}{2\\sigma^2}\\right)\n\\]\nThat can be entirely expressed in terms of two sufficient statistics!\nWe can now generate some samples from a normal distribution and see if this posterior can help us recover the true estimates.\n\nmu &lt;- 0\nsigma &lt;- 3\nn &lt;- 50\ny &lt;- rnorm(n,mu,sigma)\nmu_range &lt;- seq(-5,5,length = 50)\ns_range &lt;- seq(0.001,5,length = 30)\npost_approx &lt;- function(y,n,mu_range,s_range){\n  mu_y &lt;- mean(y)\n  s_y &lt;- var(y)\n  post &lt;- matrix(0,nrow = length(mu_range),ncol = length(s_range))\n  for( i in 1:length(mu_range)){\n    for(j in 1:length(s_range)){\n      post[i,j] &lt;- (s_range[j]^(-(n+2)))*exp(-((n-1)*s_y+n*(mu_y-mu_range[i])^2)\n                                             /(2*s_range[j]^2))\n    }\n  }\n  return(post)\n}\n\n\n\n\n\nlibrary(fields)\n\nWarning: package 'fields' was built under R version 4.2.3\n\n\nLoading required package: spam\n\n\nWarning: package 'spam' was built under R version 4.2.3\n\n\nSpam version 2.10-0 (2023-10-23) is loaded.\nType 'help( Spam)' or 'demo( spam)' for a short introduction \nand overview of this package.\nHelp for individual functions is also obtained by adding the\nsuffix '.spam' to the function name, e.g. 'help( chol.spam)'.\n\n\n\nAttaching package: 'spam'\n\n\nThe following objects are masked from 'package:base':\n\n    backsolve, forwardsolve\n\n\nLoading required package: viridisLite\n\n\nWarning: package 'viridisLite' was built under R version 4.2.3\n\n\n\nTry help(fields) to get started.\n\nfor(n1 in seq(5,n,by =  15)){\n  estimates &lt;- post_approx(y[1:n1],n1,mu_range,s_range)\n  #Normalize Posteriors as densities are quite small\n  estimates &lt;- estimates/sum(estimates)\n\n\n  image.plot(mu_range,s_range,estimates,\n           xlab=expression(theta),\n           ylab=expression(sigma),\n           main = paste(n1,\" Samples\"))\n  #contour(mu_range,s_range,estimates,add=T)\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe can see that our posterior does a reasonable job of picking up the true parameters from the data when using an improper prior once enough samples are included.\n\n10.1.3.1 Example #3 Marginal Posterior Distribution - Mean\nWe can now marginalize the posterior to be able to get the marginal posteriors for \\(\\theta\\) and \\(\\sigma^2\\). In practice this is quite simple (usually). All that is required is to integrate the full posterior over the parameter space of the nuisance parameter (in this case, \\(\\sigma^2\\).\n\\[\np(\\theta|y) = \\int_0^\\infty p(\\theta,\\sigma^2|y)d\\sigma^2\n\\]\n\\[\n= \\int_0^\\infty p(y|\\theta,\\sigma^2)p(\\theta,\\sigma^2)d\\sigma^2\n\\]\nIn this setting, we define the joint prior as follows:\n\\[\np(\\theta,\\sigma^2) = p(\\theta |\\sigma^2)p(\\sigma^2)\n\\]\nWhere\n\\[\np(\\theta |\\sigma^2) \\sim Normal(\\mu,\\sigma^2/\\lambda) = \\frac{1}{\\sqrt{2\\pi \\frac{\\sigma^2}{\\lambda}}} \\exp\\left(-\\frac{(\\theta - \\mu)^2}{2 \\frac{\\sigma^2}{\\lambda}}\\right)\n\\]\n\\[\np(\\sigma^2) \\sim InvGamma(\\alpha,\\beta) = \\frac{\\beta^\\alpha}{\\Gamma(\\alpha)} (\\sigma^2)^{-(\\alpha + 1)} exp\\left({-\\frac{\\beta}{\\sigma^2}}\\right)\n\\]\n\\[\np(\\theta,\\sigma^2)  \\propto\n\\]\nThe full posterior is therefore\n\\[\np(\\theta,\\sigma^2 | y) \\propto  \n\\]\n\\[\np(\\theta|y) = \\int_0^\\infty p(\\theta,\\sigma^2|y)d\\sigma^2 \\propto \\int_0^\\infty\n\\]"
  },
  {
    "objectID": "MoreModels.html#hierarchical-models",
    "href": "MoreModels.html#hierarchical-models",
    "title": "10  More Models!",
    "section": "10.2 Hierarchical Models",
    "text": "10.2 Hierarchical Models"
  },
  {
    "objectID": "Main.html#acknowledgements",
    "href": "Main.html#acknowledgements",
    "title": "2  Introduction",
    "section": "2.2 Acknowledgements",
    "text": "2.2 Acknowledgements\nThe projects and notes I take will be heavily based on work I found on the following pages:\n\nhttps://vioshyvo.github.io/Bayesian_inference/index.html\nhttps://www.math.mcgill.ca/dstephens/556/\nhttps://www.math.mcgill.ca/dstephens/557/\nhttps://www.math.mcgill.ca/yyang/comp.html\nhttps://www.math.mcgill.ca/dstephens/559/"
  },
  {
    "objectID": "BayesianModels.html#regression-models",
    "href": "BayesianModels.html#regression-models",
    "title": "9  Bayesian Modelling",
    "section": "9.1 Regression Models",
    "text": "9.1 Regression Models\nWe can consider an infinite sequence \\(\\{ (X_n,Y_n), n = 1,2,…\\}\\) such that for any \\(n \\geq 1\\)\n\\[\nf_{X_1,...,X_n,Y_1,...,Y_n}(x_1,...,x_n,y_1,...,y_n)\n\\]\ncan be factorized as\n\\[\nf_{X_1,...,X_n}(x_1,...,x_n)f_{Y_1,...,Y_n|X_1,...,X_n}(y_1,...,y_n|x_1,...,x_n)\n\\]\nwhere each term has a deFinetti representation.\n\\[\nf_{X_1,...,X_n}(x_1,...,x_n) = \\int \\{\\prod_{i=1}^nf_X(x_i;\\phi)\\}\\pi_0(d\\phi) \\\\\nf_{Y_1,...,Y_n|X_1,...,X_n}(y_1,...,y_n|x_1,...,x_n) = \\int \\{\\prod_{i=1}^nf_{Y|X}(y_i|x_i;\\theta)\\}\\pi_0(d\\theta)\n\\]\nGiven the above structure, inference for \\((\\phi,\\theta)\\) is required:\n\ninference for \\(\\phi\\) is done through the marginal model for the X variables\ninference for \\(\\theta\\) is done through the conditional model for Y given that X is observed.\n\nFor the latter, the fact that X is random is irrelevant as we have conditioned the model on observed values of X.\nWhen considering the statistical behaviour of Bayesian (and frequentist) procedures, we need to remember that X and Y have a joint structure.\nPrediction\n\\[\nf_{Y_{n+1}|X_{1:n},Y_{1:n}}(y_{n+1}|x_{1:n},y_{1:n}) \\\\\n= \\int f_{X_{n+1},Y_{n+1}|X_{1:n},Y_{1:n}}(x_{n+1},y_{n+1}|x_{1:n},y_{1:n})dx_{n+1} \\\\\n= \\int f_{Y_{n+1}|X_{1:n},X_{n+1},Y_{1:n}}(y_{n+1}|x_{1:n},x_{n+1},y_{1:n})f_{X_{n+1}|X_{1:n},Y_{1:n}}(x_{n+1}|x_{1:n},y_{1:n})dx_{n+1}\n\\]"
  }
]